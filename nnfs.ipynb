{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1-cdw0hlFay",
        "outputId": "db9c8f5b-2949-41af-bf30-f0225f5bbe60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nnfs in /opt/homebrew/lib/python3.10/site-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from nnfs) (2.2.6)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nnfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.10/site-packages (3.10.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/ameygupta/Library/Python/3.10/lib/python/site-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib) (12.1.1)\n",
            "Requirement already satisfied: pyparsing>=3 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/ameygupta/Library/Python/3.10/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ameygupta/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sBShBmLpl_3",
        "outputId": "ab0f0272-e017-4b54-d753-0ecac3a19c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.06414769 0.17437149 0.47399085 0.28748998]\n",
            " [0.04517666 0.90739747 0.00224921 0.04517666]\n",
            " [0.00522984 0.34875873 0.63547983 0.0105316 ]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([1., 1., 1.])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()\n",
        "inputs = [[1, 2, 3, 2.5],\n",
        "          [2, 5, -1, 2],\n",
        "          [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "print(probabilities)\n",
        "np.sum(probabilities, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uGMo39z-qveE"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLu:\n",
        "   def forward(self, inputs):\n",
        "    self.output = np.maximum(0, inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fDvKAYAwp60o"
      },
      "outputs": [],
      "source": [
        "class Layer_Dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights = 0.01*np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d-mr6hu7o1qT"
      },
      "outputs": [],
      "source": [
        "class Activation_Softmax:\n",
        "  def forward(self, inputs):\n",
        "\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "    probabilites = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    self.output = probabilites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ix6Hi03phVU",
        "outputId": "ed96b8c5-4c13-456b-b8ca-7131d29bbc92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333316 0.3333332  0.33333364]\n",
            " [0.33333287 0.3333329  0.33333418]\n",
            " [0.3333326  0.33333263 0.33333477]\n",
            " [0.33333233 0.3333324  0.33333528]]\n"
          ]
        }
      ],
      "source": [
        "X, Y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "activation1 = Activation_ReLu()\n",
        "\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(activation2.output[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ky8Mo7-taWH",
        "outputId": "20cede9c-a150-4c36-9aee-d90bc5a7bb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7 0.5 0.9]\n"
          ]
        }
      ],
      "source": [
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
        "                          [0.1, 0.5, 0.4],\n",
        "                          [0.02, 0.9, 0.08]])\n",
        "class_targets = [0, 1, 1]\n",
        "print(softmax_outputs[[0, 1, 2], class_targets])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK62Cn64TJe-",
        "outputId": "d7c0f5cf-d8ec-4ccd-a088-0876967fb2f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.35667494 0.69314718 0.10536052]\n",
            "0.38506088005216804\n"
          ]
        }
      ],
      "source": [
        "print(-np.log(softmax_outputs[\n",
        "    range(len(softmax_outputs)), class_targets]))\n",
        "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
        "average_loss = np.mean(neg_log)\n",
        "print(average_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXsYr-xYXaCn",
        "outputId": "fc7be529-b807-4395-f808-b417c5e483e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.35667494 0.69314718 0.10536052]\n",
            "0.38506088005216804\n"
          ]
        }
      ],
      "source": [
        "y_true_check = np.array([\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 0],\n",
        "    [0, 1, 0]\n",
        "])\n",
        "y_pred_clipped_check = np.array([\n",
        "    [0.7, 0.2, 0.1],\n",
        "    [0.1, 0.5, 0.4],\n",
        "    [0.02, 0.9, 0.08]\n",
        "])\n",
        "\n",
        "A = y_true_check*y_pred_clipped_check\n",
        "B = np.sum(A, axis=1)\n",
        "C = -np.log(B)\n",
        "\n",
        "print(C)\n",
        "print(np.mean(C))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DzoEGGbXmj_I"
      },
      "outputs": [],
      "source": [
        "class Loss:\n",
        "\n",
        "  def calculate(self, output, y):\n",
        "    sample_losses = self.forward(output, y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4Pwf2Xm3YAR2"
      },
      "outputs": [],
      "source": [
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "    def forward(self, y_pred, y_true):\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T_UEO_Amwrk",
        "outputId": "29907832-2edf-4fd1-b9a9-718022c490c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.38506088005216804\n"
          ]
        }
      ],
      "source": [
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
        "  [0.1, 0.5, 0.4],\n",
        "  [0.02, 0.9, 0.08]])\n",
        "class_targets = np.array([[1, 0, 0],\n",
        "  [0, 1, 0],\n",
        "  [0, 1, 0]])\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An5L9nProe18",
        "outputId": "3a221fcf-dcd4-4a0c-cd8a-749112080eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 1.0986081\n",
            "acc: 0.33666666666666667\n"
          ]
        }
      ],
      "source": [
        "X, Y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "activation1 = Activation_ReLu()\n",
        "\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "loss = loss_function.calculate(activation2.output, Y)\n",
        "print('loss:', loss)\n",
        "\n",
        "predictions = np.argmax(activation2.output, axis=1)\n",
        "if len(Y.shape) == 2:\n",
        "  Y = np.argmax(Y, axis=1)\n",
        "accuracy = np.mean(predictions==Y)\n",
        "print(\"acc:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "lvjwCSLcH_8P",
        "outputId": "f1a5b6e3-e606-45bf-901b-96dd93ed2705"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nnfs\n",
        "from nnfs.datasets import vertical_data\n",
        "nnfs.init()\n",
        "X, Y = vertical_data(samples=100, classes=3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=Y, s=40, cmap='brg')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OgbL6XaIxwY",
        "outputId": "78c31a14-a2b8-442d-afb3-7fa1258bd4b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New set of weights found, iteration: 0 loss: 1.1016203 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 1 loss: 1.1002508 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 2 loss: 1.0992025 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 3 loss: 1.0986239 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 10 loss: 1.0984299 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 22 loss: 1.0976521 acc: 0.36333333333333334\n",
            "New set of weights found, iteration: 150 loss: 1.0974255 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 874 loss: 1.0972673 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 894 loss: 1.096895 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 1036 loss: 1.095428 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 88633 loss: 1.0952065 acc: 0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "X, Y = vertical_data(samples=100, classes=3)\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "activation1 = Activation_ReLu()\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "activation2 = Activation_Softmax()\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "lowest_loss = 9999999\n",
        "best_dense1_weights = dense1.weights.copy()\n",
        "best_dense1_biases = dense1.biases.copy()\n",
        "best_dense2_weights = dense2.weights.copy()\n",
        "best_dense2_biases = dense2.biases.copy()\n",
        "\n",
        "for iteration in range(100000):\n",
        "  dense1.weights = 0.05*np.random.randn(2, 3)\n",
        "  dense1.biases = 0.05*np.random.randn(1, 3)\n",
        "  dense2.weights = 0.05*np.random.randn(3, 3)\n",
        "  dense2.biases = 0.05*np.random.randn(1, 3)\n",
        "\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  loss = loss_function.calculate(activation2.output, Y)\n",
        "  predictions = np.argmax(activation2.output, axis=1)\n",
        "  accuracy = np.mean(predictions == Y)\n",
        "\n",
        "  if loss < lowest_loss:\n",
        "    print('New set of weights found, iteration:', iteration, 'loss:', loss, 'acc:', accuracy)\n",
        "    best_dense1_weights = dense1.weights.copy()\n",
        "    best_dense1_biases = dense1.biases.copy()\n",
        "    best_dense2_weights = dense2.weights.copy()\n",
        "    best_dense2_weights = dense2.biases.copy()\n",
        "    lowest_loss = loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New set of weights found, iteration: 0 loss: 1.1008747 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 3 loss: 1.1005714 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 4 loss: 1.099462 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 9 loss: 1.0994359 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 10 loss: 1.09855 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 13 loss: 1.098517 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 14 loss: 1.0938607 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 15 loss: 1.0920315 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 17 loss: 1.091391 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 19 loss: 1.0910357 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 20 loss: 1.0898421 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 21 loss: 1.0843327 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 26 loss: 1.0835577 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 27 loss: 1.0823517 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 28 loss: 1.0778279 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 31 loss: 1.076321 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 35 loss: 1.0729524 acc: 0.3\n",
            "New set of weights found, iteration: 36 loss: 1.0699975 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 47 loss: 1.0631136 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 52 loss: 1.062574 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 53 loss: 1.0624933 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 54 loss: 1.0592291 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 57 loss: 1.0574998 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 58 loss: 1.0493913 acc: 0.38\n",
            "New set of weights found, iteration: 64 loss: 1.0450443 acc: 0.65\n",
            "New set of weights found, iteration: 68 loss: 1.0377563 acc: 0.65\n",
            "New set of weights found, iteration: 70 loss: 1.0333902 acc: 0.66\n",
            "New set of weights found, iteration: 71 loss: 1.0315365 acc: 0.63\n",
            "New set of weights found, iteration: 74 loss: 1.0283976 acc: 0.6033333333333334\n",
            "New set of weights found, iteration: 75 loss: 1.0214951 acc: 0.6133333333333333\n",
            "New set of weights found, iteration: 82 loss: 1.0101981 acc: 0.6466666666666666\n",
            "New set of weights found, iteration: 84 loss: 1.0100108 acc: 0.5733333333333334\n",
            "New set of weights found, iteration: 90 loss: 1.0035288 acc: 0.7133333333333334\n",
            "New set of weights found, iteration: 92 loss: 1.0008268 acc: 0.65\n",
            "New set of weights found, iteration: 95 loss: 0.99175525 acc: 0.6\n",
            "New set of weights found, iteration: 98 loss: 0.9812336 acc: 0.66\n",
            "New set of weights found, iteration: 101 loss: 0.97777444 acc: 0.58\n",
            "New set of weights found, iteration: 106 loss: 0.977026 acc: 0.67\n",
            "New set of weights found, iteration: 110 loss: 0.97546613 acc: 0.6766666666666666\n",
            "New set of weights found, iteration: 111 loss: 0.9656759 acc: 0.9066666666666666\n",
            "New set of weights found, iteration: 115 loss: 0.96165353 acc: 0.8733333333333333\n",
            "New set of weights found, iteration: 116 loss: 0.94877195 acc: 0.9\n",
            "New set of weights found, iteration: 122 loss: 0.93169373 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 127 loss: 0.9187146 acc: 0.81\n",
            "New set of weights found, iteration: 131 loss: 0.91708404 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 136 loss: 0.9127953 acc: 0.6933333333333334\n",
            "New set of weights found, iteration: 139 loss: 0.90787965 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 141 loss: 0.9043988 acc: 0.66\n",
            "New set of weights found, iteration: 146 loss: 0.90065634 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 147 loss: 0.89855623 acc: 0.6533333333333333\n",
            "New set of weights found, iteration: 149 loss: 0.8932796 acc: 0.69\n",
            "New set of weights found, iteration: 152 loss: 0.87403554 acc: 0.84\n",
            "New set of weights found, iteration: 153 loss: 0.8707889 acc: 0.7666666666666667\n",
            "New set of weights found, iteration: 156 loss: 0.8667261 acc: 0.88\n",
            "New set of weights found, iteration: 158 loss: 0.8490861 acc: 0.87\n",
            "New set of weights found, iteration: 162 loss: 0.84764665 acc: 0.8333333333333334\n",
            "New set of weights found, iteration: 165 loss: 0.8428589 acc: 0.8033333333333333\n",
            "New set of weights found, iteration: 166 loss: 0.8239612 acc: 0.66\n",
            "New set of weights found, iteration: 167 loss: 0.7897708 acc: 0.7\n",
            "New set of weights found, iteration: 171 loss: 0.7849403 acc: 0.6733333333333333\n",
            "New set of weights found, iteration: 172 loss: 0.782472 acc: 0.6633333333333333\n",
            "New set of weights found, iteration: 173 loss: 0.7803537 acc: 0.6633333333333333\n",
            "New set of weights found, iteration: 175 loss: 0.7747049 acc: 0.67\n",
            "New set of weights found, iteration: 178 loss: 0.77419984 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 179 loss: 0.75924987 acc: 0.7233333333333334\n",
            "New set of weights found, iteration: 183 loss: 0.7520049 acc: 0.67\n",
            "New set of weights found, iteration: 187 loss: 0.7399888 acc: 0.7366666666666667\n",
            "New set of weights found, iteration: 188 loss: 0.73904145 acc: 0.8333333333333334\n",
            "New set of weights found, iteration: 190 loss: 0.7320137 acc: 0.8266666666666667\n",
            "New set of weights found, iteration: 195 loss: 0.712903 acc: 0.69\n",
            "New set of weights found, iteration: 198 loss: 0.7052826 acc: 0.7266666666666667\n",
            "New set of weights found, iteration: 199 loss: 0.70251924 acc: 0.6966666666666667\n",
            "New set of weights found, iteration: 200 loss: 0.69334394 acc: 0.72\n",
            "New set of weights found, iteration: 201 loss: 0.6803275 acc: 0.79\n",
            "New set of weights found, iteration: 205 loss: 0.6770639 acc: 0.7666666666666667\n",
            "New set of weights found, iteration: 206 loss: 0.66997415 acc: 0.8733333333333333\n",
            "New set of weights found, iteration: 207 loss: 0.6599941 acc: 0.8066666666666666\n",
            "New set of weights found, iteration: 208 loss: 0.65543926 acc: 0.7766666666666666\n",
            "New set of weights found, iteration: 209 loss: 0.62252766 acc: 0.8033333333333333\n",
            "New set of weights found, iteration: 210 loss: 0.61185175 acc: 0.7766666666666666\n",
            "New set of weights found, iteration: 211 loss: 0.599142 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 213 loss: 0.5920534 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 222 loss: 0.5918576 acc: 0.8533333333333334\n",
            "New set of weights found, iteration: 229 loss: 0.57633215 acc: 0.8533333333333334\n",
            "New set of weights found, iteration: 234 loss: 0.55926883 acc: 0.8933333333333333\n",
            "New set of weights found, iteration: 239 loss: 0.5579059 acc: 0.83\n",
            "New set of weights found, iteration: 243 loss: 0.55486155 acc: 0.8466666666666667\n",
            "New set of weights found, iteration: 246 loss: 0.5507333 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 247 loss: 0.545519 acc: 0.8833333333333333\n",
            "New set of weights found, iteration: 248 loss: 0.5346363 acc: 0.86\n",
            "New set of weights found, iteration: 264 loss: 0.5342746 acc: 0.83\n",
            "New set of weights found, iteration: 265 loss: 0.5253426 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 267 loss: 0.51280546 acc: 0.9066666666666666\n",
            "New set of weights found, iteration: 269 loss: 0.50337905 acc: 0.9233333333333333\n",
            "New set of weights found, iteration: 276 loss: 0.5024293 acc: 0.89\n",
            "New set of weights found, iteration: 278 loss: 0.491296 acc: 0.9133333333333333\n",
            "New set of weights found, iteration: 283 loss: 0.48956776 acc: 0.9033333333333333\n",
            "New set of weights found, iteration: 284 loss: 0.48599878 acc: 0.93\n",
            "New set of weights found, iteration: 289 loss: 0.48009065 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 290 loss: 0.47792414 acc: 0.9233333333333333\n",
            "New set of weights found, iteration: 291 loss: 0.46914592 acc: 0.9133333333333333\n",
            "New set of weights found, iteration: 298 loss: 0.46764258 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 300 loss: 0.45708776 acc: 0.8933333333333333\n",
            "New set of weights found, iteration: 301 loss: 0.45304757 acc: 0.8966666666666666\n",
            "New set of weights found, iteration: 304 loss: 0.45023417 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 308 loss: 0.44732147 acc: 0.8833333333333333\n",
            "New set of weights found, iteration: 309 loss: 0.44072458 acc: 0.88\n",
            "New set of weights found, iteration: 313 loss: 0.4319237 acc: 0.8933333333333333\n",
            "New set of weights found, iteration: 318 loss: 0.42440805 acc: 0.92\n",
            "New set of weights found, iteration: 320 loss: 0.41862 acc: 0.92\n",
            "New set of weights found, iteration: 324 loss: 0.41297048 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 326 loss: 0.41245985 acc: 0.9\n",
            "New set of weights found, iteration: 328 loss: 0.40676734 acc: 0.9233333333333333\n",
            "New set of weights found, iteration: 329 loss: 0.40640786 acc: 0.93\n",
            "New set of weights found, iteration: 335 loss: 0.39842996 acc: 0.93\n",
            "New set of weights found, iteration: 338 loss: 0.39804557 acc: 0.94\n",
            "New set of weights found, iteration: 344 loss: 0.39646825 acc: 0.9233333333333333\n",
            "New set of weights found, iteration: 347 loss: 0.38945505 acc: 0.93\n",
            "New set of weights found, iteration: 348 loss: 0.3871968 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 351 loss: 0.3779128 acc: 0.9133333333333333\n",
            "New set of weights found, iteration: 354 loss: 0.37311223 acc: 0.9233333333333333\n",
            "New set of weights found, iteration: 355 loss: 0.37276617 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 356 loss: 0.36852098 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 360 loss: 0.35868368 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 362 loss: 0.35691482 acc: 0.93\n",
            "New set of weights found, iteration: 367 loss: 0.3429358 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 368 loss: 0.33983532 acc: 0.93\n",
            "New set of weights found, iteration: 375 loss: 0.33676398 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 378 loss: 0.33293056 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 379 loss: 0.32861033 acc: 0.93\n",
            "New set of weights found, iteration: 380 loss: 0.32313567 acc: 0.93\n",
            "New set of weights found, iteration: 383 loss: 0.32154855 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 386 loss: 0.32056552 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 392 loss: 0.32019016 acc: 0.93\n",
            "New set of weights found, iteration: 394 loss: 0.31832498 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 400 loss: 0.315471 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 401 loss: 0.3055538 acc: 0.93\n",
            "New set of weights found, iteration: 407 loss: 0.3045038 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 410 loss: 0.30153093 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 417 loss: 0.29838136 acc: 0.92\n",
            "New set of weights found, iteration: 420 loss: 0.29762506 acc: 0.93\n",
            "New set of weights found, iteration: 428 loss: 0.29648978 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 434 loss: 0.29306 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 435 loss: 0.29192674 acc: 0.93\n",
            "New set of weights found, iteration: 440 loss: 0.28488693 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 444 loss: 0.28334972 acc: 0.93\n",
            "New set of weights found, iteration: 448 loss: 0.28309777 acc: 0.93\n",
            "New set of weights found, iteration: 449 loss: 0.28108674 acc: 0.93\n",
            "New set of weights found, iteration: 453 loss: 0.27773702 acc: 0.94\n",
            "New set of weights found, iteration: 461 loss: 0.27312914 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 467 loss: 0.2729932 acc: 0.93\n",
            "New set of weights found, iteration: 472 loss: 0.27115387 acc: 0.93\n",
            "New set of weights found, iteration: 473 loss: 0.26995963 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 475 loss: 0.26944205 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 477 loss: 0.268688 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 479 loss: 0.26242334 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 493 loss: 0.26207444 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 495 loss: 0.25908068 acc: 0.93\n",
            "New set of weights found, iteration: 496 loss: 0.2590734 acc: 0.93\n",
            "New set of weights found, iteration: 497 loss: 0.2582146 acc: 0.94\n",
            "New set of weights found, iteration: 503 loss: 0.25407296 acc: 0.93\n",
            "New set of weights found, iteration: 505 loss: 0.25202662 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 506 loss: 0.24973544 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 512 loss: 0.24969201 acc: 0.93\n",
            "New set of weights found, iteration: 517 loss: 0.24860601 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 519 loss: 0.24686712 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 529 loss: 0.24602742 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 535 loss: 0.2445193 acc: 0.93\n",
            "New set of weights found, iteration: 536 loss: 0.24081425 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 538 loss: 0.24043368 acc: 0.93\n",
            "New set of weights found, iteration: 546 loss: 0.23582341 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 550 loss: 0.23467208 acc: 0.93\n",
            "New set of weights found, iteration: 552 loss: 0.23393926 acc: 0.93\n",
            "New set of weights found, iteration: 557 loss: 0.22943695 acc: 0.93\n",
            "New set of weights found, iteration: 564 loss: 0.22614151 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 570 loss: 0.22351934 acc: 0.93\n",
            "New set of weights found, iteration: 582 loss: 0.2227324 acc: 0.93\n",
            "New set of weights found, iteration: 585 loss: 0.21830775 acc: 0.93\n",
            "New set of weights found, iteration: 597 loss: 0.2167703 acc: 0.93\n",
            "New set of weights found, iteration: 610 loss: 0.21555844 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 625 loss: 0.21515213 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 629 loss: 0.21122937 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 631 loss: 0.21094893 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 637 loss: 0.21082413 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 659 loss: 0.21066985 acc: 0.93\n",
            "New set of weights found, iteration: 661 loss: 0.20929192 acc: 0.93\n",
            "New set of weights found, iteration: 684 loss: 0.20894809 acc: 0.93\n",
            "New set of weights found, iteration: 686 loss: 0.20748574 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 689 loss: 0.205267 acc: 0.93\n",
            "New set of weights found, iteration: 708 loss: 0.20465927 acc: 0.93\n",
            "New set of weights found, iteration: 712 loss: 0.20393105 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 729 loss: 0.20358147 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 733 loss: 0.20317748 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 739 loss: 0.20279907 acc: 0.94\n",
            "New set of weights found, iteration: 740 loss: 0.20236613 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 746 loss: 0.2021528 acc: 0.93\n",
            "New set of weights found, iteration: 748 loss: 0.20100321 acc: 0.93\n",
            "New set of weights found, iteration: 754 loss: 0.1993275 acc: 0.93\n",
            "New set of weights found, iteration: 757 loss: 0.19792211 acc: 0.93\n",
            "New set of weights found, iteration: 773 loss: 0.19783711 acc: 0.93\n",
            "New set of weights found, iteration: 775 loss: 0.19740187 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 776 loss: 0.19721994 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 777 loss: 0.1961473 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 779 loss: 0.19598241 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 784 loss: 0.19526182 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 785 loss: 0.19456321 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 791 loss: 0.1943641 acc: 0.93\n",
            "New set of weights found, iteration: 799 loss: 0.19334234 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 806 loss: 0.19311096 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 809 loss: 0.19277118 acc: 0.93\n",
            "New set of weights found, iteration: 811 loss: 0.19264112 acc: 0.93\n",
            "New set of weights found, iteration: 815 loss: 0.190967 acc: 0.93\n",
            "New set of weights found, iteration: 890 loss: 0.18890005 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 892 loss: 0.18846218 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 893 loss: 0.18713883 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 903 loss: 0.1869782 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 909 loss: 0.18646298 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 917 loss: 0.18598829 acc: 0.93\n",
            "New set of weights found, iteration: 927 loss: 0.18594755 acc: 0.93\n",
            "New set of weights found, iteration: 932 loss: 0.18573302 acc: 0.93\n",
            "New set of weights found, iteration: 942 loss: 0.18538505 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 946 loss: 0.18424016 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 959 loss: 0.1835666 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 969 loss: 0.1830834 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 974 loss: 0.18286093 acc: 0.93\n",
            "New set of weights found, iteration: 983 loss: 0.1820501 acc: 0.93\n",
            "New set of weights found, iteration: 985 loss: 0.18183175 acc: 0.93\n",
            "New set of weights found, iteration: 993 loss: 0.18173474 acc: 0.93\n",
            "New set of weights found, iteration: 995 loss: 0.18133913 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1001 loss: 0.18080577 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1006 loss: 0.1802216 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1025 loss: 0.17995203 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1042 loss: 0.17973644 acc: 0.93\n",
            "New set of weights found, iteration: 1061 loss: 0.17944387 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1082 loss: 0.17943431 acc: 0.93\n",
            "New set of weights found, iteration: 1083 loss: 0.17871827 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1093 loss: 0.17833588 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1123 loss: 0.1782659 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1144 loss: 0.17753273 acc: 0.93\n",
            "New set of weights found, iteration: 1154 loss: 0.17727007 acc: 0.93\n",
            "New set of weights found, iteration: 1165 loss: 0.17724434 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1166 loss: 0.17719936 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1171 loss: 0.17672187 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1176 loss: 0.17671774 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1212 loss: 0.17666677 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1223 loss: 0.17643958 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1275 loss: 0.1762153 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1303 loss: 0.17542142 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1378 loss: 0.17535225 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1382 loss: 0.17529377 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1404 loss: 0.17457567 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1457 loss: 0.17447841 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1461 loss: 0.17424846 acc: 0.93\n",
            "New set of weights found, iteration: 1477 loss: 0.1737377 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1548 loss: 0.17335981 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1580 loss: 0.1733058 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1587 loss: 0.17321306 acc: 0.93\n",
            "New set of weights found, iteration: 1596 loss: 0.17308939 acc: 0.93\n",
            "New set of weights found, iteration: 1614 loss: 0.17301595 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1644 loss: 0.17284796 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1647 loss: 0.17258313 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1662 loss: 0.1723816 acc: 0.93\n",
            "New set of weights found, iteration: 1706 loss: 0.17223743 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 1719 loss: 0.1719875 acc: 0.93\n",
            "New set of weights found, iteration: 1978 loss: 0.17198084 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 2001 loss: 0.17195481 acc: 0.93\n",
            "New set of weights found, iteration: 2026 loss: 0.1717552 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 2031 loss: 0.17165588 acc: 0.93\n",
            "New set of weights found, iteration: 2066 loss: 0.17161626 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 2074 loss: 0.17140518 acc: 0.93\n",
            "New set of weights found, iteration: 2441 loss: 0.17138883 acc: 0.93\n",
            "New set of weights found, iteration: 2636 loss: 0.17134853 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 2718 loss: 0.17130204 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 2775 loss: 0.17122428 acc: 0.93\n",
            "New set of weights found, iteration: 2829 loss: 0.1711681 acc: 0.93\n",
            "New set of weights found, iteration: 2874 loss: 0.17114341 acc: 0.93\n",
            "New set of weights found, iteration: 2978 loss: 0.17100853 acc: 0.93\n",
            "New set of weights found, iteration: 3138 loss: 0.17093514 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 3293 loss: 0.17080545 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 3486 loss: 0.1707664 acc: 0.93\n",
            "New set of weights found, iteration: 3681 loss: 0.17070974 acc: 0.93\n",
            "New set of weights found, iteration: 4038 loss: 0.17066137 acc: 0.93\n",
            "New set of weights found, iteration: 4090 loss: 0.17065905 acc: 0.93\n",
            "New set of weights found, iteration: 4116 loss: 0.17063397 acc: 0.93\n",
            "New set of weights found, iteration: 4258 loss: 0.17062972 acc: 0.93\n",
            "New set of weights found, iteration: 5960 loss: 0.17062148 acc: 0.93\n",
            "New set of weights found, iteration: 6352 loss: 0.17058212 acc: 0.93\n",
            "New set of weights found, iteration: 8192 loss: 0.17057395 acc: 0.93\n",
            "New set of weights found, iteration: 8254 loss: 0.17056267 acc: 0.93\n",
            "New set of weights found, iteration: 9463 loss: 0.17056097 acc: 0.93\n",
            "New set of weights found, iteration: 15128 loss: 0.17055875 acc: 0.93\n",
            "New set of weights found, iteration: 15988 loss: 0.17054859 acc: 0.93\n",
            "New set of weights found, iteration: 27341 loss: 0.17054608 acc: 0.93\n"
          ]
        }
      ],
      "source": [
        "X, Y = vertical_data(samples=100, classes=3)\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "activation1 = Activation_ReLu()\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "activation2 = Activation_Softmax()\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "lowest_loss = 9999999\n",
        "best_dense1_weights = dense1.weights.copy()\n",
        "best_dense1_biases = dense1.biases.copy()\n",
        "best_dense2_weights = dense2.weights.copy()\n",
        "best_dense2_biases = dense2.biases.copy()\n",
        "\n",
        "for iteration in range(100000):\n",
        "  dense1.weights += 0.05*np.random.randn(2, 3)\n",
        "  dense1.biases += 0.05*np.random.randn(1, 3)\n",
        "  dense2.weights += 0.05*np.random.randn(3, 3)\n",
        "  dense2.biases += 0.05*np.random.randn(1, 3)\n",
        "\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  loss = loss_function.calculate(activation2.output, Y)\n",
        "  predictions = np.argmax(activation2.output, axis=1)\n",
        "  accuracy = np.mean(predictions == Y)\n",
        "\n",
        "  if loss < lowest_loss:\n",
        "    print('New set of weights found, iteration:', iteration, 'loss:', loss, 'acc:', accuracy)\n",
        "    best_dense1_weights = dense1.weights.copy()\n",
        "    best_dense1_biases = dense1.biases.copy()\n",
        "    best_dense2_weights = dense2.weights.copy()\n",
        "    best_dense2_biases = dense2.biases.copy()\n",
        "    lowest_loss = loss\n",
        "  else:\n",
        "    dense1.weights = best_dense1_weights.copy()\n",
        "    dense1.biases = best_dense1_biases.copy()\n",
        "    dense2.weights = best_dense2_weights.copy()\n",
        "    dense2.biases = best_dense2_biases.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, Loss: 36.0\n",
            "Iteration 2, Loss: 33.872397424621624\n",
            "Iteration 3, Loss: 31.87054345809546\n",
            "Iteration 4, Loss: 29.98699091998773\n",
            "Iteration 5, Loss: 28.214761511794592\n",
            "Iteration 6, Loss: 26.54726775906168\n",
            "Iteration 7, Loss: 24.978326552541866\n",
            "Iteration 8, Loss: 23.5021050739742\n",
            "Iteration 9, Loss: 22.11313179151597\n",
            "Iteration 10, Loss: 20.806246424284897\n",
            "Iteration 11, Loss: 19.576596334671486\n",
            "Iteration 12, Loss: 18.41961908608719\n",
            "Iteration 13, Loss: 17.33101994032309\n",
            "Iteration 14, Loss: 16.306757070164853\n",
            "Iteration 15, Loss: 15.343027506224132\n",
            "Iteration 16, Loss: 14.436253786815284\n",
            "Iteration 17, Loss: 13.583071280700132\n",
            "Iteration 18, Loss: 12.780312744165439\n",
            "Iteration 19, Loss: 12.024995767388878\n",
            "Iteration 20, Loss: 11.314319082257104\n",
            "Iteration 21, Loss: 10.64564263994962\n",
            "Iteration 22, Loss: 10.016485041642266\n",
            "Iteration 23, Loss: 9.424510031713222\n",
            "Iteration 24, Loss: 8.867521365009814\n",
            "Iteration 25, Loss: 8.34345204094211\n",
            "Iteration 26, Loss: 7.850353118483743\n",
            "Iteration 27, Loss: 7.386397874602818\n",
            "Iteration 28, Loss: 6.94986173712617\n",
            "Iteration 29, Loss: 6.539124434950737\n",
            "Iteration 30, Loss: 6.1526621719118015\n",
            "Iteration 31, Loss: 5.789039869058961\n",
            "Iteration 32, Loss: 5.446907999417336\n",
            "Iteration 33, Loss: 5.124995576577539\n",
            "Iteration 34, Loss: 4.822108497170647\n",
            "Iteration 35, Loss: 4.537121521071987\n",
            "Iteration 36, Loss: 4.268978030723312\n",
            "Iteration 37, Loss: 4.01668121563854\n",
            "Iteration 38, Loss: 3.7792956126389763\n",
            "Iteration 39, Loss: 3.5559389510643094\n",
            "Iteration 40, Loss: 3.345782865003274\n",
            "Iteration 41, Loss: 3.1480471758404285\n",
            "Iteration 42, Loss: 2.961997679823884\n",
            "Iteration 43, Loss: 2.78694359065541\n",
            "Iteration 44, Loss: 2.622235303237792\n",
            "Iteration 45, Loss: 2.467261121418954\n",
            "Iteration 46, Loss: 2.321446092335641\n",
            "Iteration 47, Loss: 2.184248486806066\n",
            "Iteration 48, Loss: 2.0551593804914616\n",
            "Iteration 49, Loss: 1.9336995852420789\n",
            "Iteration 50, Loss: 1.8194178573235094\n",
            "Iteration 51, Loss: 1.7118903069357754\n",
            "Iteration 52, Loss: 1.6107175940030252\n",
            "Iteration 53, Loss: 1.5155241897377694\n",
            "Iteration 54, Loss: 1.4259567411109748\n",
            "Iteration 55, Loss: 1.3416826255281136\n",
            "Iteration 56, Loss: 1.262389208248047\n",
            "Iteration 57, Loss: 1.1877819791340551\n",
            "Iteration 58, Loss: 1.1175840765571434\n",
            "Iteration 59, Loss: 1.0515348500680068\n",
            "Iteration 60, Loss: 0.9893891461492582\n",
            "Iteration 61, Loss: 0.930916260625565\n",
            "Iteration 62, Loss: 0.875899078709395\n",
            "Iteration 63, Loss: 0.8241334819517507\n",
            "Iteration 64, Loss: 0.7754271861095672\n",
            "Iteration 65, Loss: 0.7295994320679934\n",
            "Iteration 66, Loss: 0.6864801042040583\n",
            "Iteration 67, Loss: 0.6459091389617334\n",
            "Iteration 68, Loss: 0.6077358933180028\n",
            "Iteration 69, Loss: 0.5718187120029812\n",
            "Iteration 70, Loss: 0.5380242202642829\n",
            "Iteration 71, Loss: 0.5062269967452033\n",
            "Iteration 72, Loss: 0.4763089781884024\n",
            "Iteration 73, Loss: 0.4481591180173807\n",
            "Iteration 74, Loss: 0.42167291418136477\n",
            "Iteration 75, Loss: 0.3967520449790852\n",
            "Iteration 76, Loss: 0.3733039992368791\n",
            "Iteration 77, Loss: 0.3512417316144445\n",
            "Iteration 78, Loss: 0.33048334753976116\n",
            "Iteration 79, Loss: 0.31095177724411444\n",
            "Iteration 80, Loss: 0.2925745286179104\n",
            "Iteration 81, Loss: 0.2752833763568879\n",
            "Iteration 82, Loss: 0.25901412505149535\n",
            "Iteration 83, Loss: 0.2437063914735247\n",
            "Iteration 84, Loss: 0.22930333977371198\n",
            "Iteration 85, Loss: 0.21575151284725816\n",
            "Iteration 86, Loss: 0.2030006012946216\n",
            "Iteration 87, Loss: 0.19100326852350488\n",
            "Iteration 88, Loss: 0.17971497196649536\n",
            "Iteration 89, Loss: 0.1690938194815031\n",
            "Iteration 90, Loss: 0.1591003719214838\n",
            "Iteration 91, Loss: 0.14969754273736763\n",
            "Iteration 92, Loss: 0.14085041966208015\n",
            "Iteration 93, Loss: 0.13252615564761738\n",
            "Iteration 94, Loss: 0.1246938532452423\n",
            "Iteration 95, Loss: 0.11732446503349986\n",
            "Iteration 96, Loss: 0.11039058885430607\n",
            "Iteration 97, Loss: 0.10386649785129919\n",
            "Iteration 98, Loss: 0.09772798570124883\n",
            "Iteration 99, Loss: 0.09195226348280558\n",
            "Iteration 100, Loss: 0.0865178816583512\n",
            "Iteration 101, Loss: 0.08140467291758889\n",
            "Iteration 102, Loss: 0.07659366262828358\n",
            "Iteration 103, Loss: 0.07206697005843195\n",
            "Iteration 104, Loss: 0.06780781192053903\n",
            "Iteration 105, Loss: 0.06380037696069592\n",
            "Iteration 106, Loss: 0.06002977345222309\n",
            "Iteration 107, Loss: 0.0564820075507719\n",
            "Iteration 108, Loss: 0.05314393144118542\n",
            "Iteration 109, Loss: 0.050003114234231524\n",
            "Iteration 110, Loss: 0.04704793686603195\n",
            "Iteration 111, Loss: 0.04426740148833972\n",
            "Iteration 112, Loss: 0.04165120020443161\n",
            "Iteration 113, Loss: 0.03918961375201954\n",
            "Iteration 114, Loss: 0.0368735034129829\n",
            "Iteration 115, Loss: 0.034694277992582755\n",
            "Iteration 116, Loss: 0.032643851730490094\n",
            "Iteration 117, Loss: 0.03071459534999028\n",
            "Iteration 118, Loss: 0.028899363239415818\n",
            "Iteration 119, Loss: 0.027191414181739672\n",
            "Iteration 120, Loss: 0.02558439994540113\n",
            "Iteration 121, Loss: 0.024072362337913877\n",
            "Iteration 122, Loss: 0.022649683089386127\n",
            "Iteration 123, Loss: 0.021311092099735786\n",
            "Iteration 124, Loss: 0.02005160424149179\n",
            "Iteration 125, Loss: 0.01886655505507656\n",
            "Iteration 126, Loss: 0.017751540667355833\n",
            "Iteration 127, Loss: 0.016702427744061103\n",
            "Iteration 128, Loss: 0.01571531497821091\n",
            "Iteration 129, Loss: 0.014786535770396103\n",
            "Iteration 130, Loss: 0.013912651762769943\n",
            "Iteration 131, Loss: 0.013090418519936803\n",
            "Iteration 132, Loss: 0.012316768931710837\n",
            "Iteration 133, Loss: 0.011588849600126475\n",
            "Iteration 134, Loss: 0.010903943586632107\n",
            "Iteration 135, Loss: 0.010259526183227799\n",
            "Iteration 136, Loss: 0.009653186757193668\n",
            "Iteration 137, Loss: 0.009082688171817357\n",
            "Iteration 138, Loss: 0.008545899068542421\n",
            "Iteration 139, Loss: 0.00804083320361364\n",
            "Iteration 140, Loss: 0.007565618804557518\n",
            "Iteration 141, Loss: 0.007118492429622391\n",
            "Iteration 142, Loss: 0.006697793120481266\n",
            "Iteration 143, Loss: 0.0063019473730584336\n",
            "Iteration 144, Loss: 0.005929501997799936\n",
            "Iteration 145, Loss: 0.005579070290327091\n",
            "Iteration 146, Loss: 0.005249347396309216\n",
            "Iteration 147, Loss: 0.004939114136252681\n",
            "Iteration 148, Loss: 0.004647215154254898\n",
            "Iteration 149, Loss: 0.00437256400626425\n",
            "Iteration 150, Loss: 0.004114139259196158\n",
            "Iteration 151, Loss: 0.0038709956233987848\n",
            "Iteration 152, Loss: 0.0036422222163822442\n",
            "Iteration 153, Loss: 0.0034269635873455254\n",
            "Iteration 154, Loss: 0.0032244300300798123\n",
            "Iteration 155, Loss: 0.003033866206344064\n",
            "Iteration 156, Loss: 0.0028545694817259646\n",
            "Iteration 157, Loss: 0.0026858615040063873\n",
            "Iteration 158, Loss: 0.002527124440860861\n",
            "Iteration 159, Loss: 0.002377772426750458\n",
            "Iteration 160, Loss: 0.0022372501846465924\n",
            "Iteration 161, Loss: 0.002105026221950533\n",
            "Iteration 162, Loss: 0.0019806188966821317\n",
            "Iteration 163, Loss: 0.001863566163059441\n",
            "Iteration 164, Loss: 0.0017534302886055876\n",
            "Iteration 165, Loss: 0.0016498016244949178\n",
            "Iteration 166, Loss: 0.0015522968336895225\n",
            "Iteration 167, Loss: 0.0014605572212372654\n",
            "Iteration 168, Loss: 0.0013742383231737623\n",
            "Iteration 169, Loss: 0.0012930183418168389\n",
            "Iteration 170, Loss: 0.0012166008279945002\n",
            "Iteration 171, Loss: 0.0011447005613673634\n",
            "Iteration 172, Loss: 0.0010770513341135804\n",
            "Iteration 173, Loss: 0.001013397095948145\n",
            "Iteration 174, Loss: 0.0009535029620325111\n",
            "Iteration 175, Loss: 0.0008971534673183893\n",
            "Iteration 176, Loss: 0.0008441301639000644\n",
            "Iteration 177, Loss: 0.0007942435095401501\n",
            "Iteration 178, Loss: 0.0007473036766382048\n",
            "Iteration 179, Loss: 0.0007031374518087182\n",
            "Iteration 180, Loss: 0.0006615806720993984\n",
            "Iteration 181, Loss: 0.0006224808039162045\n",
            "Iteration 182, Loss: 0.0005856932236775429\n",
            "Iteration 183, Loss: 0.0005510780772974099\n",
            "Iteration 184, Loss: 0.0005185112321657664\n",
            "Iteration 185, Loss: 0.00048786689510026934\n",
            "Iteration 186, Loss: 0.00045903387854597503\n",
            "Iteration 187, Loss: 0.00043190420223823955\n",
            "Iteration 188, Loss: 0.000406378034681195\n",
            "Iteration 189, Loss: 0.00038236074013664776\n",
            "Iteration 190, Loss: 0.0003597649139507893\n",
            "Iteration 191, Loss: 0.0003385032407062897\n",
            "Iteration 192, Loss: 0.00031849748027454767\n",
            "Iteration 193, Loss: 0.00029967346881992795\n",
            "Iteration 194, Loss: 0.0002819629431575354\n",
            "Iteration 195, Loss: 0.0002652991815966534\n",
            "Iteration 196, Loss: 0.00024961903501571355\n",
            "Iteration 197, Loss: 0.00023486641976601822\n",
            "Iteration 198, Loss: 0.00022098629075865584\n",
            "Iteration 199, Loss: 0.00020792651372860275\n",
            "Iteration 200, Loss: 0.00019563773612380077\n",
            "Iteration 201, Loss: 0.00018407487969713572\n",
            "Iteration 202, Loss: 0.0001731967742579334\n",
            "Iteration 203, Loss: 0.000162961252717828\n",
            "Iteration 204, Loss: 0.00015332995845652932\n",
            "Iteration 205, Loss: 0.0001442680116302569\n",
            "Iteration 206, Loss: 0.00013574231108876236\n",
            "Iteration 207, Loss: 0.00012772011195796273\n",
            "Iteration 208, Loss: 0.00012017177102625989\n",
            "Iteration 209, Loss: 0.00011306919742051021\n",
            "Iteration 210, Loss: 0.00010638701417807839\n",
            "Iteration 211, Loss: 0.00010009872759385047\n",
            "Iteration 212, Loss: 9.418282202748521e-05\n",
            "Iteration 213, Loss: 8.861767093682102e-05\n",
            "Iteration 214, Loss: 8.337949257687632e-05\n",
            "Iteration 215, Loss: 7.845230166985068e-05\n",
            "Iteration 216, Loss: 7.381560437745125e-05\n",
            "Iteration 217, Loss: 6.945311009593931e-05\n",
            "Iteration 218, Loss: 6.534836187948565e-05\n",
            "Iteration 219, Loss: 6.148672061729929e-05\n",
            "Iteration 220, Loss: 5.785246296650091e-05\n",
            "Iteration 221, Loss: 5.443342464548701e-05\n",
            "Iteration 222, Loss: 5.121628804567445e-05\n",
            "Iteration 223, Loss: 4.818930464094636e-05\n",
            "Iteration 224, Loss: 4.534133155345275e-05\n",
            "Iteration 225, Loss: 4.2661805568748396e-05\n",
            "Iteration 226, Loss: 4.014071796380716e-05\n",
            "Iteration 227, Loss: 3.776785753161897e-05\n",
            "Iteration 228, Loss: 3.5536451494682454e-05\n",
            "Iteration 229, Loss: 3.3435810899983596e-05\n",
            "Iteration 230, Loss: 3.145998067594679e-05\n",
            "Iteration 231, Loss: 2.960060347716992e-05\n",
            "Iteration 232, Loss: 2.7851086964507083e-05\n",
            "Iteration 233, Loss: 2.620516601559193e-05\n",
            "Iteration 234, Loss: 2.465629747782624e-05\n",
            "Iteration 235, Loss: 2.3199459697225655e-05\n",
            "Iteration 236, Loss: 2.1828175193711868e-05\n",
            "Iteration 237, Loss: 2.0537999306060552e-05\n",
            "Iteration 238, Loss: 1.932415559459508e-05\n",
            "Iteration 239, Loss: 1.818211765893713e-05\n",
            "Iteration 240, Loss: 1.710759819061359e-05\n",
            "Iteration 241, Loss: 1.6096538382966948e-05\n",
            "Iteration 242, Loss: 1.5145561622815826e-05\n",
            "Iteration 243, Loss: 1.4250093077000611e-05\n",
            "Iteration 244, Loss: 1.3408051662561137e-05\n",
            "Iteration 245, Loss: 1.261569000514266e-05\n",
            "Iteration 246, Loss: 1.1869908764341794e-05\n",
            "Iteration 247, Loss: 1.1168579891738937e-05\n",
            "Iteration 248, Loss: 1.050848504269381e-05\n",
            "Iteration 249, Loss: 9.887365329824825e-06\n",
            "Iteration 250, Loss: 9.303068687509335e-06\n",
            "Iteration 251, Loss: 8.753193152616391e-06\n",
            "Iteration 252, Loss: 8.235820732913284e-06\n",
            "Iteration 253, Loss: 7.749126451891957e-06\n",
            "Iteration 254, Loss: 7.291374698706425e-06\n",
            "Iteration 255, Loss: 6.86029118952526e-06\n",
            "Iteration 256, Loss: 6.454971598994486e-06\n",
            "Iteration 257, Loss: 6.073338043191166e-06\n",
            "Iteration 258, Loss: 5.714314948706091e-06\n",
            "Iteration 259, Loss: 5.376861244181049e-06\n",
            "Iteration 260, Loss: 5.058897583687484e-06\n",
            "Iteration 261, Loss: 4.760069790385164e-06\n",
            "Iteration 262, Loss: 4.478726279706828e-06\n",
            "Iteration 263, Loss: 4.214072096117582e-06\n",
            "Iteration 264, Loss: 3.964863098941029e-06\n",
            "Iteration 265, Loss: 3.7306291413026542e-06\n",
            "Iteration 266, Loss: 3.5102243872401033e-06\n",
            "Iteration 267, Loss: 3.302581007251695e-06\n",
            "Iteration 268, Loss: 3.1075453155103137e-06\n",
            "Iteration 269, Loss: 2.9239112786960698e-06\n",
            "Iteration 270, Loss: 2.750963226299029e-06\n",
            "Iteration 271, Loss: 2.588409808223717e-06\n",
            "Iteration 272, Loss: 2.4353935533070035e-06\n",
            "Iteration 273, Loss: 2.2914796215798234e-06\n",
            "Iteration 274, Loss: 2.1560721842999686e-06\n",
            "Iteration 275, Loss: 2.028610688774251e-06\n",
            "Iteration 276, Loss: 1.9087327782973714e-06\n",
            "Iteration 277, Loss: 1.7959280548825464e-06\n",
            "Iteration 278, Loss: 1.6898721775203354e-06\n",
            "Iteration 279, Loss: 1.5899507618576355e-06\n",
            "Iteration 280, Loss: 1.4960339883200618e-06\n",
            "Iteration 281, Loss: 1.4075586324292928e-06\n",
            "Iteration 282, Loss: 1.3244169701464047e-06\n",
            "Iteration 283, Loss: 1.2460930592493338e-06\n",
            "Iteration 284, Loss: 1.1724991084167118e-06\n",
            "Iteration 285, Loss: 1.1031628082226808e-06\n",
            "Iteration 286, Loss: 1.038014272451196e-06\n",
            "Iteration 287, Loss: 9.766214005942013e-07\n",
            "Iteration 288, Loss: 9.189303169384961e-07\n",
            "Iteration 289, Loss: 8.646567745431894e-07\n",
            "Iteration 290, Loss: 8.135359116362026e-07\n",
            "Iteration 291, Loss: 7.654254470605013e-07\n",
            "Iteration 292, Loss: 7.201876115518505e-07\n",
            "Iteration 293, Loss: 6.77590987483452e-07\n",
            "Iteration 294, Loss: 6.376109370344866e-07\n",
            "Iteration 295, Loss: 5.999380612327986e-07\n",
            "Iteration 296, Loss: 5.644689442400999e-07\n",
            "Iteration 297, Loss: 5.311034103738157e-07\n",
            "Iteration 298, Loss: 4.997445036648104e-07\n",
            "Iteration 299, Loss: 4.701349782917526e-07\n",
            "Iteration 300, Loss: 4.423578405331198e-07\n",
            "Iteration 301, Loss: 4.162482304912002e-07\n",
            "Iteration 302, Loss: 3.916537642960803e-07\n",
            "Iteration 303, Loss: 3.685061327710813e-07\n",
            "Iteration 304, Loss: 3.4673918996799673e-07\n",
            "Iteration 305, Loss: 3.2622084868208e-07\n",
            "Iteration 306, Loss: 3.06961545222367e-07\n",
            "Iteration 307, Loss: 2.88773190833825e-07\n",
            "Iteration 308, Loss: 2.717339715321837e-07\n",
            "Iteration 309, Loss: 2.5566714431358216e-07\n",
            "Iteration 310, Loss: 2.405881066617867e-07\n",
            "Iteration 311, Loss: 2.2633629896608824e-07\n",
            "Iteration 312, Loss: 2.129866938005535e-07\n",
            "Iteration 313, Loss: 2.0038837105345607e-07\n",
            "Iteration 314, Loss: 1.8856031865155106e-07\n",
            "Iteration 315, Loss: 1.7741576493418525e-07\n",
            "Iteration 316, Loss: 1.6692403015157863e-07\n",
            "Iteration 317, Loss: 1.5705539800922388e-07\n",
            "Iteration 318, Loss: 1.4778110958859831e-07\n",
            "Iteration 319, Loss: 1.390289046777176e-07\n",
            "Iteration 320, Loss: 1.3081911746291563e-07\n",
            "Iteration 321, Loss: 1.2308385511998542e-07\n",
            "Iteration 322, Loss: 1.1580181320184772e-07\n",
            "Iteration 323, Loss: 1.089523563226927e-07\n",
            "Iteration 324, Loss: 1.025155139361648e-07\n",
            "Iteration 325, Loss: 9.647197613420246e-08\n",
            "Iteration 326, Loss: 9.076717102739713e-08\n",
            "Iteration 327, Loss: 8.538638825941643e-08\n",
            "Iteration 328, Loss: 8.034913987144744e-08\n",
            "Iteration 329, Loss: 7.560569746024702e-08\n",
            "Iteration 330, Loss: 7.11427448077312e-08\n",
            "Iteration 331, Loss: 6.694739188212724e-08\n",
            "Iteration 332, Loss: 6.297725270130486e-08\n",
            "Iteration 333, Loss: 5.9252048490398297e-08\n",
            "Iteration 334, Loss: 5.576005574382907e-08\n",
            "Iteration 335, Loss: 5.2462658953262825e-08\n",
            "Iteration 336, Loss: 4.9351461876236805e-08\n",
            "Iteration 337, Loss: 4.6444000796088087e-08\n",
            "Iteration 338, Loss: 4.3705086740845364e-08\n",
            "Iteration 339, Loss: 4.110295646366446e-08\n",
            "Iteration 340, Loss: 3.867937298668705e-08\n",
            "Iteration 341, Loss: 3.6402256365788405e-08\n",
            "Iteration 342, Loss: 3.4242669017720546e-08\n",
            "Iteration 343, Loss: 3.221739542043817e-08\n",
            "Iteration 344, Loss: 3.0319959328803195e-08\n",
            "Iteration 345, Loss: 2.852398095013049e-08\n",
            "Iteration 346, Loss: 2.6844821225101486e-08\n",
            "Iteration 347, Loss: 2.5257679259917202e-08\n",
            "Iteration 348, Loss: 2.3758660463245398e-08\n",
            "Iteration 349, Loss: 2.236180319343551e-08\n",
            "Iteration 350, Loss: 2.1044501813968934e-08\n",
            "Iteration 351, Loss: 1.9786466983252944e-08\n",
            "Iteration 352, Loss: 1.861836829377517e-08\n",
            "Iteration 353, Loss: 1.751957892185943e-08\n",
            "Iteration 354, Loss: 1.6486902872669676e-08\n",
            "Iteration 355, Loss: 1.5517249857305253e-08\n",
            "Iteration 356, Loss: 1.4593230297763548e-08\n",
            "Iteration 357, Loss: 1.3727256118487875e-08\n",
            "Iteration 358, Loss: 1.2916502388601935e-08\n",
            "Iteration 359, Loss: 1.2158247534822206e-08\n",
            "Iteration 360, Loss: 1.1437120356723651e-08\n",
            "Iteration 361, Loss: 1.0764135551630628e-08\n",
            "Iteration 362, Loss: 1.01248319138835e-08\n",
            "Iteration 363, Loss: 9.529578690887409e-09\n",
            "Iteration 364, Loss: 8.96477736354114e-09\n",
            "Iteration 365, Loss: 8.440195906235816e-09\n",
            "Iteration 366, Loss: 7.932430544102242e-09\n",
            "Iteration 367, Loss: 7.472266832278253e-09\n",
            "Iteration 368, Loss: 7.026731145259329e-09\n",
            "Iteration 369, Loss: 6.615116626024153e-09\n",
            "Iteration 370, Loss: 6.216710537455385e-09\n",
            "Iteration 371, Loss: 5.849657741952001e-09\n",
            "Iteration 372, Loss: 5.503313621377447e-09\n",
            "Iteration 373, Loss: 5.185353021776613e-09\n",
            "Iteration 374, Loss: 4.877460591601938e-09\n",
            "Iteration 375, Loss: 4.58764925651727e-09\n",
            "Iteration 376, Loss: 4.315093953097227e-09\n",
            "Iteration 377, Loss: 4.058996209609811e-09\n",
            "Iteration 378, Loss: 3.81858397796227e-09\n",
            "Iteration 379, Loss: 3.5931114665422146e-09\n",
            "Iteration 380, Loss: 3.3818589737682034e-09\n",
            "Iteration 381, Loss: 3.1841327225691324e-09\n",
            "Iteration 382, Loss: 2.992739686842292e-09\n",
            "Iteration 383, Loss: 2.8202908168843075e-09\n",
            "Iteration 384, Loss: 2.6532884835845445e-09\n",
            "Iteration 385, Loss: 2.4976555239962114e-09\n",
            "Iteration 386, Loss: 2.3470231679832496e-09\n",
            "Iteration 387, Loss: 2.2069593711529577e-09\n",
            "Iteration 388, Loss: 2.0769008272453913e-09\n",
            "Iteration 389, Loss: 1.9563091602347835e-09\n",
            "Iteration 390, Loss: 1.8395543222426357e-09\n",
            "Iteration 391, Loss: 1.7315712738788423e-09\n",
            "Iteration 392, Loss: 1.6270603189314375e-09\n",
            "Iteration 393, Loss: 1.5306618543705572e-09\n",
            "Iteration 394, Loss: 1.441914012896908e-09\n",
            "Iteration 395, Loss: 1.355985926738931e-09\n",
            "Iteration 396, Loss: 1.2771180022860584e-09\n",
            "Iteration 397, Loss: 1.2007635479054962e-09\n",
            "Iteration 398, Loss: 1.1309134664129285e-09\n",
            "Iteration 399, Loss: 1.0632897799183128e-09\n",
            "Iteration 400, Loss: 1.0016489800461908e-09\n",
            "Iteration 401, Loss: 9.419664030264834e-10\n",
            "Iteration 402, Loss: 8.842307999948089e-10\n",
            "Iteration 403, Loss: 8.318656652143516e-10\n",
            "Iteration 404, Loss: 7.845341603917397e-10\n",
            "Iteration 405, Loss: 7.386794234065891e-10\n",
            "Iteration 406, Loss: 6.942929778358266e-10\n",
            "Iteration 407, Loss: 6.544123842334545e-10\n",
            "Iteration 408, Loss: 6.15787524235985e-10\n",
            "Iteration 409, Loss: 5.784111871932369e-10\n",
            "Iteration 410, Loss: 5.450557541713469e-10\n",
            "Iteration 411, Loss: 5.127545946166009e-10\n",
            "Iteration 412, Loss: 4.815016536835224e-10\n",
            "Iteration 413, Loss: 4.5382689260383933e-10\n",
            "Iteration 414, Loss: 4.2702405342147915e-10\n",
            "Iteration 415, Loss: 4.01088129269621e-10\n",
            "Iteration 416, Loss: 3.78329286391731e-10\n",
            "Iteration 417, Loss: 3.540321003146379e-10\n",
            "Iteration 418, Loss: 3.3493656118966156e-10\n",
            "Iteration 419, Loss: 3.142900599444268e-10\n",
            "Iteration 420, Loss: 2.9638838894938204e-10\n",
            "Iteration 421, Loss: 2.7904583716985216e-10\n",
            "Iteration 422, Loss: 2.622591951984347e-10\n",
            "Iteration 423, Loss: 2.460252684030117e-10\n",
            "Iteration 424, Loss: 2.3215366718614213e-10\n",
            "Iteration 425, Loss: 2.1871109162951433e-10\n",
            "Iteration 426, Loss: 2.0569508015919812e-10\n",
            "Iteration 427, Loss: 1.9310318253084939e-10\n",
            "Iteration 428, Loss: 1.809329597788117e-10\n",
            "Iteration 429, Loss: 1.707360918233375e-10\n",
            "Iteration 430, Loss: 1.6085445786117078e-10\n",
            "Iteration 431, Loss: 1.5128624924310202e-10\n",
            "Iteration 432, Loss: 1.4202966564330687e-10\n",
            "Iteration 433, Loss: 1.3446168480619637e-10\n",
            "Iteration 434, Loss: 1.2577492653275457e-10\n",
            "Iteration 435, Loss: 1.1868989061567061e-10\n",
            "Iteration 436, Loss: 1.118237894289504e-10\n",
            "Iteration 437, Loss: 1.0517536686250857e-10\n",
            "Iteration 438, Loss: 9.874337259029887e-11\n",
            "Iteration 439, Loss: 9.25265620433259e-11\n",
            "Iteration 440, Loss: 8.763611143217675e-11\n",
            "Iteration 441, Loss: 8.180605623757667e-11\n",
            "Iteration 442, Loss: 7.723179356290995e-11\n",
            "Iteration 443, Loss: 7.279787752439727e-11\n",
            "Iteration 444, Loss: 6.85035031604929e-11\n",
            "Iteration 445, Loss: 6.434786921732239e-11\n",
            "Iteration 446, Loss: 6.033017812563737e-11\n",
            "Iteration 447, Loss: 5.734884353693717e-11\n",
            "Iteration 448, Loss: 5.357270202691421e-11\n",
            "Iteration 449, Loss: 5.0778236046888876e-11\n",
            "Iteration 450, Loss: 4.724107868767589e-11\n",
            "Iteration 451, Loss: 4.4631140784112767e-11\n",
            "Iteration 452, Loss: 4.2100359416188153e-11\n",
            "Iteration 453, Loss: 3.9648280818041924e-11\n",
            "Iteration 454, Loss: 3.727445331506208e-11\n",
            "Iteration 455, Loss: 3.497842730932261e-11\n",
            "Iteration 456, Loss: 3.2759755276236596e-11\n",
            "Iteration 457, Loss: 3.128117150447912e-11\n",
            "Iteration 458, Loss: 2.919194935762766e-11\n",
            "Iteration 459, Loss: 2.7803920071949916e-11\n",
            "Iteration 460, Loss: 2.584281043106892e-11\n",
            "Iteration 461, Loss: 2.4544156953992527e-11\n",
            "Iteration 462, Loss: 2.270983432237923e-11\n",
            "Iteration 463, Loss: 2.1499389201047443e-11\n",
            "Iteration 464, Loss: 2.0324413699077835e-11\n",
            "Iteration 465, Loss: 1.9184704814879942e-11\n",
            "Iteration 466, Loss: 1.808006048028799e-11\n",
            "Iteration 467, Loss: 1.701027955664106e-11\n",
            "Iteration 468, Loss: 1.597516183087629e-11\n",
            "Iteration 469, Loss: 1.4974508012494145e-11\n",
            "Iteration 470, Loss: 1.4008119728705434e-11\n",
            "Iteration 471, Loss: 1.3075799520563665e-11\n",
            "Iteration 472, Loss: 1.2596897018591866e-11\n",
            "Iteration 473, Loss: 1.1716265282090823e-11\n",
            "Iteration 474, Loss: 1.0869206426477006e-11\n",
            "Iteration 475, Loss: 1.0437096918718226e-11\n",
            "Iteration 476, Loss: 9.640894737439782e-12\n",
            "Iteration 477, Loss: 9.236520377287499e-12\n",
            "Iteration 478, Loss: 8.490678020221989e-12\n",
            "Iteration 479, Loss: 8.113635523387327e-12\n",
            "Iteration 480, Loss: 7.745884237611702e-12\n",
            "Iteration 481, Loss: 7.066916676098494e-12\n",
            "Iteration 482, Loss: 6.72596171817692e-12\n",
            "Iteration 483, Loss: 6.394092969544031e-12\n",
            "Iteration 484, Loss: 6.071258673659406e-12\n",
            "Iteration 485, Loss: 5.757407312986827e-12\n",
            "Iteration 486, Loss: 5.1776799244647295e-12\n",
            "Iteration 487, Loss: 4.889830171855265e-12\n",
            "Iteration 488, Loss: 4.610763308470204e-12\n",
            "Iteration 489, Loss: 4.340428972329695e-12\n",
            "Iteration 490, Loss: 4.078777033430322e-12\n",
            "Iteration 491, Loss: 3.825757593169392e-12\n",
            "Iteration 492, Loss: 3.5813209815869976e-12\n",
            "Iteration 493, Loss: 3.5670100231207092e-12\n",
            "Iteration 494, Loss: 3.331614276574586e-12\n",
            "Iteration 495, Loss: 3.10469977741694e-12\n",
            "Iteration 496, Loss: 2.8862175531998803e-12\n",
            "Iteration 497, Loss: 2.676118857512189e-12\n",
            "Iteration 498, Loss: 2.4743551689893284e-12\n",
            "Iteration 499, Loss: 2.46446764563017e-12\n",
            "Iteration 500, Loss: 2.271404461318318e-12\n",
            "Iteration 501, Loss: 2.0865775097379336e-12\n",
            "Iteration 502, Loss: 2.0782395458681763e-12\n",
            "Iteration 503, Loss: 1.901978010674013e-12\n",
            "Iteration 504, Loss: 1.7338550521970592e-12\n",
            "Iteration 505, Loss: 1.7269265673378654e-12\n",
            "Iteration 506, Loss: 1.5672357936268467e-12\n",
            "Iteration 507, Loss: 1.4155870463536374e-12\n",
            "Iteration 508, Loss: 1.4099303605928687e-12\n",
            "Iteration 509, Loss: 1.266582399149767e-12\n",
            "Iteration 510, Loss: 1.261521135931147e-12\n",
            "Iteration 511, Loss: 1.1264077532906628e-12\n",
            "Iteration 512, Loss: 1.1219066279127466e-12\n",
            "Iteration 513, Loss: 9.949621444876733e-13\n",
            "Iteration 514, Loss: 9.909862758126768e-13\n",
            "Iteration 515, Loss: 8.721455364691638e-13\n",
            "Iteration 516, Loss: 8.686604429969049e-13\n",
            "Iteration 517, Loss: 7.578588117529039e-13\n",
            "Iteration 518, Loss: 7.548304079870529e-13\n",
            "Iteration 519, Loss: 7.518141057418114e-13\n",
            "Iteration 520, Loss: 6.492062419222522e-13\n",
            "Iteration 521, Loss: 6.466120137398926e-13\n",
            "Iteration 522, Loss: 5.51913784237351e-13\n",
            "Iteration 523, Loss: 5.497083367236006e-13\n",
            "Iteration 524, Loss: 5.475117021587915e-13\n",
            "Iteration 525, Loss: 4.608452127441061e-13\n",
            "Iteration 526, Loss: 4.590036752917319e-13\n",
            "Iteration 527, Loss: 4.57169496569234e-13\n",
            "Iteration 528, Loss: 3.7845402704433747e-13\n",
            "Iteration 529, Loss: 3.7694172473481863e-13\n",
            "Iteration 530, Loss: 3.754354656645463e-13\n",
            "Iteration 531, Loss: 3.739352255934466e-13\n",
            "Iteration 532, Loss: 3.0324271699375317e-13\n",
            "Iteration 533, Loss: 3.02030959096158e-13\n",
            "Iteration 534, Loss: 3.0082404336485477e-13\n",
            "Iteration 535, Loss: 2.9962195053326866e-13\n",
            "Iteration 536, Loss: 2.3685541615154836e-13\n",
            "Iteration 537, Loss: 2.359089419448437e-13\n",
            "Iteration 538, Loss: 2.34966249803791e-13\n",
            "Iteration 539, Loss: 2.340273246227412e-13\n",
            "Iteration 540, Loss: 1.7909107551509862e-13\n",
            "Iteration 541, Loss: 1.7837542757377667e-13\n",
            "Iteration 542, Loss: 1.7766263935396084e-13\n",
            "Iteration 543, Loss: 1.769526994743766e-13\n",
            "Iteration 544, Loss: 1.7624559650014104e-13\n",
            "Iteration 545, Loss: 1.2914814025794405e-13\n",
            "Iteration 546, Loss: 1.286320642944108e-13\n",
            "Iteration 547, Loss: 1.2811805059140006e-13\n",
            "Iteration 548, Loss: 1.2760609089978574e-13\n",
            "Iteration 549, Loss: 1.2709617699933868e-13\n",
            "Iteration 550, Loss: 1.2658830069864382e-13\n",
            "Iteration 551, Loss: 8.730622430015422e-14\n",
            "Iteration 552, Loss: 8.695734864238685e-14\n",
            "Iteration 553, Loss: 8.660986704898208e-14\n",
            "Iteration 554, Loss: 8.626377403355843e-14\n",
            "Iteration 555, Loss: 8.591906399837803e-14\n",
            "Iteration 556, Loss: 8.55757314304624e-14\n",
            "Iteration 557, Loss: 8.523377083605986e-14\n",
            "Iteration 558, Loss: 5.3712554781314075e-14\n",
            "Iteration 559, Loss: 5.349791943541636e-14\n",
            "Iteration 560, Loss: 5.3284141771187215e-14\n",
            "Iteration 561, Loss: 5.307121834598945e-14\n",
            "Iteration 562, Loss: 5.285914578034951e-14\n",
            "Iteration 563, Loss: 5.264792065551527e-14\n",
            "Iteration 564, Loss: 5.243753956477921e-14\n",
            "Iteration 565, Loss: 5.2227999164188595e-14\n",
            "Iteration 566, Loss: 5.2019296070716555e-14\n",
            "Iteration 567, Loss: 2.8229574842332852e-14\n",
            "Iteration 568, Loss: 2.8116769454784423e-14\n",
            "Iteration 569, Loss: 2.800441485853476e-14\n",
            "Iteration 570, Loss: 2.7892509219059253e-14\n",
            "Iteration 571, Loss: 2.778105074541012e-14\n",
            "Iteration 572, Loss: 2.7670037652891423e-14\n",
            "Iteration 573, Loss: 2.7559468199902925e-14\n",
            "Iteration 574, Loss: 2.7449340577189728e-14\n",
            "Iteration 575, Loss: 2.7339653018705018e-14\n",
            "Iteration 576, Loss: 2.7230403764582734e-14\n",
            "Iteration 577, Loss: 2.7121591061119908e-14\n",
            "Iteration 578, Loss: 2.7013213197253592e-14\n",
            "Iteration 579, Loss: 2.690526839491354e-14\n",
            "Iteration 580, Loss: 1.0835884283733627e-14\n",
            "Iteration 581, Loss: 1.0792584085199355e-14\n",
            "Iteration 582, Loss: 1.0749456927206378e-14\n",
            "Iteration 583, Loss: 1.0706502106791147e-14\n",
            "Iteration 584, Loss: 1.0663718923501088e-14\n",
            "Iteration 585, Loss: 1.0621106702270873e-14\n",
            "Iteration 586, Loss: 1.0578664770348343e-14\n",
            "Iteration 587, Loss: 1.0536392434495898e-14\n",
            "Iteration 588, Loss: 1.0494289003958601e-14\n",
            "Iteration 589, Loss: 1.0452353835859113e-14\n",
            "Iteration 590, Loss: 1.0410586221457782e-14\n",
            "Iteration 591, Loss: 1.036898552248928e-14\n",
            "Iteration 592, Loss: 1.0327551057650194e-14\n",
            "Iteration 593, Loss: 1.028628217060504e-14\n",
            "Iteration 594, Loss: 1.0245178184803801e-14\n",
            "Iteration 595, Loss: 1.0204238448560473e-14\n",
            "Iteration 596, Loss: 1.0163462312436352e-14\n",
            "Iteration 597, Loss: 1.0122849106893299e-14\n",
            "Iteration 598, Loss: 1.0082398209398118e-14\n",
            "Iteration 599, Loss: 1.0042108954964823e-14\n",
            "Iteration 600, Loss: 1.6325848853041835e-15\n",
            "Iteration 601, Loss: 1.6260610798810225e-15\n",
            "Iteration 602, Loss: 1.6195633368569648e-15\n",
            "Iteration 603, Loss: 1.6130915602289804e-15\n",
            "Iteration 604, Loss: 1.6066456453930827e-15\n",
            "Iteration 605, Loss: 1.600225488115025e-15\n",
            "Iteration 606, Loss: 1.5938309845292286e-15\n",
            "Iteration 607, Loss: 1.5874620399846312e-15\n",
            "Iteration 608, Loss: 1.5811185424674826e-15\n",
            "Iteration 609, Loss: 1.5748003892119108e-15\n",
            "Iteration 610, Loss: 1.5685074866103897e-15\n",
            "Iteration 611, Loss: 1.562239732572034e-15\n",
            "Iteration 612, Loss: 1.555997025368457e-15\n",
            "Iteration 613, Loss: 1.5497792636327134e-15\n",
            "Iteration 614, Loss: 1.5435863463582459e-15\n",
            "Iteration 615, Loss: 1.5374181728978324e-15\n",
            "Iteration 616, Loss: 1.5312746516514764e-15\n",
            "Iteration 617, Loss: 1.5251556739637844e-15\n",
            "Iteration 618, Loss: 1.519061148913574e-15\n",
            "Iteration 619, Loss: 1.5129909772286681e-15\n",
            "Iteration 620, Loss: 1.5069450686108418e-15\n",
            "Iteration 621, Loss: 1.5009233158413962e-15\n",
            "Iteration 622, Loss: 1.4949256292932816e-15\n",
            "Iteration 623, Loss: 1.4889519024859276e-15\n",
            "Iteration 624, Loss: 1.4830020464594562e-15\n",
            "Iteration 625, Loss: 1.4770759725347232e-15\n",
            "Iteration 626, Loss: 1.4711735752791981e-15\n",
            "Iteration 627, Loss: 1.465294766675345e-15\n",
            "Iteration 628, Loss: 1.4594394505012124e-15\n",
            "Iteration 629, Loss: 1.4536075308802954e-15\n",
            "Iteration 630, Loss: 1.4477989122805152e-15\n",
            "Iteration 631, Loss: 1.4420135079450886e-15\n",
            "Iteration 632, Loss: 1.4362512229771431e-15\n",
            "Iteration 633, Loss: 1.4305119628213842e-15\n",
            "Iteration 634, Loss: 1.4247956332630833e-15\n",
            "Iteration 635, Loss: 1.4191021487917027e-15\n",
            "Iteration 636, Loss: 1.4134314158204279e-15\n",
            "Iteration 637, Loss: 1.4077833411001848e-15\n",
            "Iteration 638, Loss: 1.4021578400331853e-15\n",
            "Iteration 639, Loss: 1.3965548199929386e-15\n",
            "Iteration 640, Loss: 1.3909741886878875e-15\n",
            "Iteration 641, Loss: 1.385415854160409e-15\n",
            "Iteration 642, Loss: 1.3798797330340497e-15\n",
            "Iteration 643, Loss: 1.3743657339665785e-15\n",
            "Iteration 644, Loss: 1.3688737659469137e-15\n",
            "Iteration 645, Loss: 1.3634037464929736e-15\n",
            "Iteration 646, Loss: 1.3579555852038122e-15\n",
            "Iteration 647, Loss: 1.352529192006864e-15\n",
            "Iteration 648, Loss: 1.347124485306706e-15\n",
            "Iteration 649, Loss: 1.341741375635684e-15\n",
            "Iteration 650, Loss: 1.3363797738517718e-15\n",
            "Iteration 651, Loss: 1.331039599238539e-15\n",
            "Iteration 652, Loss: 1.3257207632536773e-15\n",
            "Iteration 653, Loss: 1.3204231857463451e-15\n",
            "Iteration 654, Loss: 1.3151467707181988e-15\n",
            "Iteration 655, Loss: 1.3098914466654538e-15\n",
            "Iteration 656, Loss: 0.0\n",
            "Iteration 657, Loss: 0.0\n",
            "Iteration 658, Loss: 0.0\n",
            "Iteration 659, Loss: 0.0\n",
            "Iteration 660, Loss: 0.0\n",
            "Iteration 661, Loss: 0.0\n",
            "Iteration 662, Loss: 0.0\n",
            "Iteration 663, Loss: 0.0\n",
            "Iteration 664, Loss: 0.0\n",
            "Iteration 665, Loss: 0.0\n",
            "Iteration 666, Loss: 0.0\n",
            "Iteration 667, Loss: 0.0\n",
            "Iteration 668, Loss: 0.0\n",
            "Iteration 669, Loss: 0.0\n",
            "Iteration 670, Loss: 0.0\n",
            "Iteration 671, Loss: 0.0\n",
            "Iteration 672, Loss: 0.0\n",
            "Iteration 673, Loss: 0.0\n",
            "Iteration 674, Loss: 0.0\n",
            "Iteration 675, Loss: 0.0\n",
            "Iteration 676, Loss: 0.0\n",
            "Iteration 677, Loss: 0.0\n",
            "Iteration 678, Loss: 0.0\n",
            "Iteration 679, Loss: 0.0\n",
            "Iteration 680, Loss: 0.0\n",
            "Iteration 681, Loss: 0.0\n",
            "Iteration 682, Loss: 0.0\n",
            "Iteration 683, Loss: 0.0\n",
            "Iteration 684, Loss: 0.0\n",
            "Iteration 685, Loss: 0.0\n",
            "Iteration 686, Loss: 0.0\n",
            "Iteration 687, Loss: 0.0\n",
            "Iteration 688, Loss: 0.0\n",
            "Iteration 689, Loss: 0.0\n",
            "Iteration 690, Loss: 0.0\n",
            "Iteration 691, Loss: 0.0\n",
            "Iteration 692, Loss: 0.0\n",
            "Iteration 693, Loss: 0.0\n",
            "Iteration 694, Loss: 0.0\n",
            "Iteration 695, Loss: 0.0\n",
            "Iteration 696, Loss: 0.0\n",
            "Iteration 697, Loss: 0.0\n",
            "Iteration 698, Loss: 0.0\n",
            "Iteration 699, Loss: 0.0\n",
            "Iteration 700, Loss: 0.0\n",
            "Iteration 701, Loss: 0.0\n",
            "Iteration 702, Loss: 0.0\n",
            "Iteration 703, Loss: 0.0\n",
            "Iteration 704, Loss: 0.0\n",
            "Iteration 705, Loss: 0.0\n",
            "Iteration 706, Loss: 0.0\n",
            "Iteration 707, Loss: 0.0\n",
            "Iteration 708, Loss: 0.0\n",
            "Iteration 709, Loss: 0.0\n",
            "Iteration 710, Loss: 0.0\n",
            "Iteration 711, Loss: 0.0\n",
            "Iteration 712, Loss: 0.0\n",
            "Iteration 713, Loss: 0.0\n",
            "Iteration 714, Loss: 0.0\n",
            "Iteration 715, Loss: 0.0\n",
            "Iteration 716, Loss: 0.0\n",
            "Iteration 717, Loss: 0.0\n",
            "Iteration 718, Loss: 0.0\n",
            "Iteration 719, Loss: 0.0\n",
            "Iteration 720, Loss: 0.0\n",
            "Iteration 721, Loss: 0.0\n",
            "Iteration 722, Loss: 0.0\n",
            "Iteration 723, Loss: 0.0\n",
            "Iteration 724, Loss: 0.0\n",
            "Iteration 725, Loss: 0.0\n",
            "Iteration 726, Loss: 0.0\n",
            "Iteration 727, Loss: 0.0\n",
            "Iteration 728, Loss: 0.0\n",
            "Iteration 729, Loss: 0.0\n",
            "Iteration 730, Loss: 0.0\n",
            "Iteration 731, Loss: 0.0\n",
            "Iteration 732, Loss: 0.0\n",
            "Iteration 733, Loss: 0.0\n",
            "Iteration 734, Loss: 0.0\n",
            "Iteration 735, Loss: 0.0\n",
            "Iteration 736, Loss: 0.0\n",
            "Iteration 737, Loss: 0.0\n",
            "Iteration 738, Loss: 0.0\n",
            "Iteration 739, Loss: 0.0\n",
            "Iteration 740, Loss: 0.0\n",
            "Iteration 741, Loss: 0.0\n",
            "Iteration 742, Loss: 0.0\n",
            "Iteration 743, Loss: 0.0\n",
            "Iteration 744, Loss: 0.0\n",
            "Iteration 745, Loss: 0.0\n",
            "Iteration 746, Loss: 0.0\n",
            "Iteration 747, Loss: 0.0\n",
            "Iteration 748, Loss: 0.0\n",
            "Iteration 749, Loss: 0.0\n",
            "Iteration 750, Loss: 0.0\n",
            "Iteration 751, Loss: 0.0\n",
            "Iteration 752, Loss: 0.0\n",
            "Iteration 753, Loss: 0.0\n",
            "Iteration 754, Loss: 0.0\n",
            "Iteration 755, Loss: 0.0\n",
            "Iteration 756, Loss: 0.0\n",
            "Iteration 757, Loss: 0.0\n",
            "Iteration 758, Loss: 0.0\n",
            "Iteration 759, Loss: 0.0\n",
            "Iteration 760, Loss: 0.0\n",
            "Iteration 761, Loss: 0.0\n",
            "Iteration 762, Loss: 0.0\n",
            "Iteration 763, Loss: 0.0\n",
            "Iteration 764, Loss: 0.0\n",
            "Iteration 765, Loss: 0.0\n",
            "Iteration 766, Loss: 0.0\n",
            "Iteration 767, Loss: 0.0\n",
            "Iteration 768, Loss: 0.0\n",
            "Iteration 769, Loss: 0.0\n",
            "Iteration 770, Loss: 0.0\n",
            "Iteration 771, Loss: 0.0\n",
            "Iteration 772, Loss: 0.0\n",
            "Iteration 773, Loss: 0.0\n",
            "Iteration 774, Loss: 0.0\n",
            "Iteration 775, Loss: 0.0\n",
            "Iteration 776, Loss: 0.0\n",
            "Iteration 777, Loss: 0.0\n",
            "Iteration 778, Loss: 0.0\n",
            "Iteration 779, Loss: 0.0\n",
            "Iteration 780, Loss: 0.0\n",
            "Iteration 781, Loss: 0.0\n",
            "Iteration 782, Loss: 0.0\n",
            "Iteration 783, Loss: 0.0\n",
            "Iteration 784, Loss: 0.0\n",
            "Iteration 785, Loss: 0.0\n",
            "Iteration 786, Loss: 0.0\n",
            "Iteration 787, Loss: 0.0\n",
            "Iteration 788, Loss: 0.0\n",
            "Iteration 789, Loss: 0.0\n",
            "Iteration 790, Loss: 0.0\n",
            "Iteration 791, Loss: 0.0\n",
            "Iteration 792, Loss: 0.0\n",
            "Iteration 793, Loss: 0.0\n",
            "Iteration 794, Loss: 0.0\n",
            "Iteration 795, Loss: 0.0\n",
            "Iteration 796, Loss: 0.0\n",
            "Iteration 797, Loss: 0.0\n",
            "Iteration 798, Loss: 0.0\n",
            "Iteration 799, Loss: 0.0\n",
            "Iteration 800, Loss: 0.0\n",
            "Iteration 801, Loss: 0.0\n",
            "Iteration 802, Loss: 0.0\n",
            "Iteration 803, Loss: 0.0\n",
            "Iteration 804, Loss: 0.0\n",
            "Iteration 805, Loss: 0.0\n",
            "Iteration 806, Loss: 0.0\n",
            "Iteration 807, Loss: 0.0\n",
            "Iteration 808, Loss: 0.0\n",
            "Iteration 809, Loss: 0.0\n",
            "Iteration 810, Loss: 0.0\n",
            "Iteration 811, Loss: 0.0\n",
            "Iteration 812, Loss: 0.0\n",
            "Iteration 813, Loss: 0.0\n",
            "Iteration 814, Loss: 0.0\n",
            "Iteration 815, Loss: 0.0\n",
            "Iteration 816, Loss: 0.0\n",
            "Iteration 817, Loss: 0.0\n",
            "Iteration 818, Loss: 0.0\n",
            "Iteration 819, Loss: 0.0\n",
            "Iteration 820, Loss: 0.0\n",
            "Iteration 821, Loss: 0.0\n",
            "Iteration 822, Loss: 0.0\n",
            "Iteration 823, Loss: 0.0\n",
            "Iteration 824, Loss: 0.0\n",
            "Iteration 825, Loss: 0.0\n",
            "Iteration 826, Loss: 0.0\n",
            "Iteration 827, Loss: 0.0\n",
            "Iteration 828, Loss: 0.0\n",
            "Iteration 829, Loss: 0.0\n",
            "Iteration 830, Loss: 0.0\n",
            "Iteration 831, Loss: 0.0\n",
            "Iteration 832, Loss: 0.0\n",
            "Iteration 833, Loss: 0.0\n",
            "Iteration 834, Loss: 0.0\n",
            "Iteration 835, Loss: 0.0\n",
            "Iteration 836, Loss: 0.0\n",
            "Iteration 837, Loss: 0.0\n",
            "Iteration 838, Loss: 0.0\n",
            "Iteration 839, Loss: 0.0\n",
            "Iteration 840, Loss: 0.0\n",
            "Iteration 841, Loss: 0.0\n",
            "Iteration 842, Loss: 0.0\n",
            "Iteration 843, Loss: 0.0\n",
            "Iteration 844, Loss: 0.0\n",
            "Iteration 845, Loss: 0.0\n",
            "Iteration 846, Loss: 0.0\n",
            "Iteration 847, Loss: 0.0\n",
            "Iteration 848, Loss: 0.0\n",
            "Iteration 849, Loss: 0.0\n",
            "Iteration 850, Loss: 0.0\n",
            "Iteration 851, Loss: 0.0\n",
            "Iteration 852, Loss: 0.0\n",
            "Iteration 853, Loss: 0.0\n",
            "Iteration 854, Loss: 0.0\n",
            "Iteration 855, Loss: 0.0\n",
            "Iteration 856, Loss: 0.0\n",
            "Iteration 857, Loss: 0.0\n",
            "Iteration 858, Loss: 0.0\n",
            "Iteration 859, Loss: 0.0\n",
            "Iteration 860, Loss: 0.0\n",
            "Iteration 861, Loss: 0.0\n",
            "Iteration 862, Loss: 0.0\n",
            "Iteration 863, Loss: 0.0\n",
            "Iteration 864, Loss: 0.0\n",
            "Iteration 865, Loss: 0.0\n",
            "Iteration 866, Loss: 0.0\n",
            "Iteration 867, Loss: 0.0\n",
            "Iteration 868, Loss: 0.0\n",
            "Iteration 869, Loss: 0.0\n",
            "Iteration 870, Loss: 0.0\n",
            "Iteration 871, Loss: 0.0\n",
            "Iteration 872, Loss: 0.0\n",
            "Iteration 873, Loss: 0.0\n",
            "Iteration 874, Loss: 0.0\n",
            "Iteration 875, Loss: 0.0\n",
            "Iteration 876, Loss: 0.0\n",
            "Iteration 877, Loss: 0.0\n",
            "Iteration 878, Loss: 0.0\n",
            "Iteration 879, Loss: 0.0\n",
            "Iteration 880, Loss: 0.0\n",
            "Iteration 881, Loss: 0.0\n",
            "Iteration 882, Loss: 0.0\n",
            "Iteration 883, Loss: 0.0\n",
            "Iteration 884, Loss: 0.0\n",
            "Iteration 885, Loss: 0.0\n",
            "Iteration 886, Loss: 0.0\n",
            "Iteration 887, Loss: 0.0\n",
            "Iteration 888, Loss: 0.0\n",
            "Iteration 889, Loss: 0.0\n",
            "Iteration 890, Loss: 0.0\n",
            "Iteration 891, Loss: 0.0\n",
            "Iteration 892, Loss: 0.0\n",
            "Iteration 893, Loss: 0.0\n",
            "Iteration 894, Loss: 0.0\n",
            "Iteration 895, Loss: 0.0\n",
            "Iteration 896, Loss: 0.0\n",
            "Iteration 897, Loss: 0.0\n",
            "Iteration 898, Loss: 0.0\n",
            "Iteration 899, Loss: 0.0\n",
            "Iteration 900, Loss: 0.0\n",
            "Iteration 901, Loss: 0.0\n",
            "Iteration 902, Loss: 0.0\n",
            "Iteration 903, Loss: 0.0\n",
            "Iteration 904, Loss: 0.0\n",
            "Iteration 905, Loss: 0.0\n",
            "Iteration 906, Loss: 0.0\n",
            "Iteration 907, Loss: 0.0\n",
            "Iteration 908, Loss: 0.0\n",
            "Iteration 909, Loss: 0.0\n",
            "Iteration 910, Loss: 0.0\n",
            "Iteration 911, Loss: 0.0\n",
            "Iteration 912, Loss: 0.0\n",
            "Iteration 913, Loss: 0.0\n",
            "Iteration 914, Loss: 0.0\n",
            "Iteration 915, Loss: 0.0\n",
            "Iteration 916, Loss: 0.0\n",
            "Iteration 917, Loss: 0.0\n",
            "Iteration 918, Loss: 0.0\n",
            "Iteration 919, Loss: 0.0\n",
            "Iteration 920, Loss: 0.0\n",
            "Iteration 921, Loss: 0.0\n",
            "Iteration 922, Loss: 0.0\n",
            "Iteration 923, Loss: 0.0\n",
            "Iteration 924, Loss: 0.0\n",
            "Iteration 925, Loss: 0.0\n",
            "Iteration 926, Loss: 0.0\n",
            "Iteration 927, Loss: 0.0\n",
            "Iteration 928, Loss: 0.0\n",
            "Iteration 929, Loss: 0.0\n",
            "Iteration 930, Loss: 0.0\n",
            "Iteration 931, Loss: 0.0\n",
            "Iteration 932, Loss: 0.0\n",
            "Iteration 933, Loss: 0.0\n",
            "Iteration 934, Loss: 0.0\n",
            "Iteration 935, Loss: 0.0\n",
            "Iteration 936, Loss: 0.0\n",
            "Iteration 937, Loss: 0.0\n",
            "Iteration 938, Loss: 0.0\n",
            "Iteration 939, Loss: 0.0\n",
            "Iteration 940, Loss: 0.0\n",
            "Iteration 941, Loss: 0.0\n",
            "Iteration 942, Loss: 0.0\n",
            "Iteration 943, Loss: 0.0\n",
            "Iteration 944, Loss: 0.0\n",
            "Iteration 945, Loss: 0.0\n",
            "Iteration 946, Loss: 0.0\n",
            "Iteration 947, Loss: 0.0\n",
            "Iteration 948, Loss: 0.0\n",
            "Iteration 949, Loss: 0.0\n",
            "Iteration 950, Loss: 0.0\n",
            "Iteration 951, Loss: 0.0\n",
            "Iteration 952, Loss: 0.0\n",
            "Iteration 953, Loss: 0.0\n",
            "Iteration 954, Loss: 0.0\n",
            "Iteration 955, Loss: 0.0\n",
            "Iteration 956, Loss: 0.0\n",
            "Iteration 957, Loss: 0.0\n",
            "Iteration 958, Loss: 0.0\n",
            "Iteration 959, Loss: 0.0\n",
            "Iteration 960, Loss: 0.0\n",
            "Iteration 961, Loss: 0.0\n",
            "Iteration 962, Loss: 0.0\n",
            "Iteration 963, Loss: 0.0\n",
            "Iteration 964, Loss: 0.0\n",
            "Iteration 965, Loss: 0.0\n",
            "Iteration 966, Loss: 0.0\n",
            "Iteration 967, Loss: 0.0\n",
            "Iteration 968, Loss: 0.0\n",
            "Iteration 969, Loss: 0.0\n",
            "Iteration 970, Loss: 0.0\n",
            "Iteration 971, Loss: 0.0\n",
            "Iteration 972, Loss: 0.0\n",
            "Iteration 973, Loss: 0.0\n",
            "Iteration 974, Loss: 0.0\n",
            "Iteration 975, Loss: 0.0\n",
            "Iteration 976, Loss: 0.0\n",
            "Iteration 977, Loss: 0.0\n",
            "Iteration 978, Loss: 0.0\n",
            "Iteration 979, Loss: 0.0\n",
            "Iteration 980, Loss: 0.0\n",
            "Iteration 981, Loss: 0.0\n",
            "Iteration 982, Loss: 0.0\n",
            "Iteration 983, Loss: 0.0\n",
            "Iteration 984, Loss: 0.0\n",
            "Iteration 985, Loss: 0.0\n",
            "Iteration 986, Loss: 0.0\n",
            "Iteration 987, Loss: 0.0\n",
            "Iteration 988, Loss: 0.0\n",
            "Iteration 989, Loss: 0.0\n",
            "Iteration 990, Loss: 0.0\n",
            "Iteration 991, Loss: 0.0\n",
            "Iteration 992, Loss: 0.0\n",
            "Iteration 993, Loss: 0.0\n",
            "Iteration 994, Loss: 0.0\n",
            "Iteration 995, Loss: 0.0\n",
            "Iteration 996, Loss: 0.0\n",
            "Iteration 997, Loss: 0.0\n",
            "Iteration 998, Loss: 0.0\n",
            "Iteration 999, Loss: 0.0\n",
            "Iteration 1000, Loss: 0.0\n",
            "Final weights:  [-3.4 -0.2  0.8]\n",
            "Final bias:  0.6000000003572508\n"
          ]
        }
      ],
      "source": [
        "weights = np.array([-3.0, -1.0, 2.0])\n",
        "bias = 1.0\n",
        "inputs = np.array([1.0, -2.0, 3.0])\n",
        "target_output = 0.0\n",
        "learning_rate = 0.001\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "for iteration in range(1000):\n",
        "  linear_output = np.dot(weights, inputs) + bias\n",
        "  output = relu(linear_output)\n",
        "  loss = (output - target_output) ** 2\n",
        "\n",
        "  dloss_doutput = 2 * (output - target_output)\n",
        "  doutput_dlinear = relu_derivative(linear_output)\n",
        "  dlinear_dweights = inputs\n",
        "  dlinear_dbias = 1.0\n",
        "\n",
        "  dloss_dlinear = dloss_doutput * doutput_dlinear\n",
        "  dloss_dweights = dloss_dlinear * dlinear_dweights\n",
        "  dloss_dbias = dloss_dlinear * dlinear_dbias\n",
        "\n",
        "  weights -= learning_rate * dloss_dweights\n",
        "  bias -= learning_rate * dloss_dbias\n",
        "\n",
        "  print(f\"Iteration {iteration + 1}, Loss: {loss}\")\n",
        "print(\"Final weights: \", weights)\n",
        "print(\"Final bias: \", bias)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0, Loss: 466.56000000000006\n",
            "Iteration 1, Loss: 309.1408061254542\n",
            "Iteration 2, Loss: 204.83546067662922\n",
            "Iteration 3, Loss: 140.81821450601942\n",
            "Iteration 4, Loss: 108.06052391356724\n",
            "Iteration 5, Loss: 82.92305140514253\n",
            "Iteration 6, Loss: 63.633160675090004\n",
            "Iteration 7, Loss: 48.8305602695014\n",
            "Iteration 8, Loss: 37.47139717735553\n",
            "Iteration 9, Loss: 28.75465078555795\n",
            "Iteration 10, Loss: 22.065629158959357\n",
            "Iteration 11, Loss: 16.932636395893613\n",
            "Iteration 12, Loss: 14.840511673308335\n",
            "Iteration 13, Loss: 13.057336391187649\n",
            "Iteration 14, Loss: 11.488418196254344\n",
            "Iteration 15, Loss: 10.10801545931892\n",
            "Iteration 16, Loss: 8.893477739126576\n",
            "Iteration 17, Loss: 7.824872432374868\n",
            "Iteration 18, Loss: 6.884666968331393\n",
            "Iteration 19, Loss: 6.057433182939761\n",
            "Iteration 20, Loss: 5.329595763793193\n",
            "Iteration 21, Loss: 4.689212990460128\n",
            "Iteration 22, Loss: 4.125776513653082\n",
            "Iteration 23, Loss: 3.630039626837757\n",
            "Iteration 24, Loss: 3.193868551928063\n",
            "Iteration 25, Loss: 2.8101060412434578\n",
            "Iteration 26, Loss: 2.4724549711385877\n",
            "Iteration 27, Loss: 2.175374653184361\n",
            "Iteration 28, Loss: 1.9139903672496266\n",
            "Iteration 29, Loss: 1.6840127722085405\n",
            "Iteration 30, Loss: 1.4816686405121537\n",
            "Iteration 31, Loss: 1.3036372649283443\n",
            "Iteration 32, Loss: 1.1469975109598998\n",
            "Iteration 33, Loss: 1.009178813639544\n",
            "Iteration 34, Loss: 0.8879198404629061\n",
            "Iteration 35, Loss: 0.781230964221879\n",
            "Iteration 36, Loss: 0.687361416064909\n",
            "Iteration 37, Loss: 0.6047708019659352\n",
            "Iteration 38, Loss: 0.5321039933126177\n",
            "Iteration 39, Loss: 0.46816850828051826\n",
            "Iteration 40, Loss: 0.41191524253483786\n",
            "Iteration 41, Loss: 0.36242110509834946\n",
            "Iteration 42, Loss: 0.31887405787535544\n",
            "Iteration 43, Loss: 0.28055942078025997\n",
            "Iteration 44, Loss: 0.24684851936708666\n",
            "Iteration 45, Loss: 0.2171881905905976\n",
            "Iteration 46, Loss: 0.19109173816150263\n",
            "Iteration 47, Loss: 0.16813091658140744\n",
            "Iteration 48, Loss: 0.14792897656788312\n",
            "Iteration 49, Loss: 0.13015442246612102\n",
            "Iteration 50, Loss: 0.11451559482686753\n",
            "Iteration 51, Loss: 0.10075586404000357\n",
            "Iteration 52, Loss: 0.08864942755220066\n",
            "Iteration 53, Loss: 0.07799766488357181\n",
            "Iteration 54, Loss: 0.06862577988199467\n",
            "Iteration 55, Loss: 0.06037999038037394\n",
            "Iteration 56, Loss: 0.05312496609274877\n",
            "Iteration 57, Loss: 0.04674167903627192\n",
            "Iteration 58, Loss: 0.04112539248385687\n",
            "Iteration 59, Loss: 0.03618392619327084\n",
            "Iteration 60, Loss: 0.03183621475376345\n",
            "Iteration 61, Loss: 0.0280108986785826\n",
            "Iteration 62, Loss: 0.02464522470496948\n",
            "Iteration 63, Loss: 0.021683949070239072\n",
            "Iteration 64, Loss: 0.019078495659233394\n",
            "Iteration 65, Loss: 0.016786097692919062\n",
            "Iteration 66, Loss: 0.01476914802865537\n",
            "Iteration 67, Loss: 0.012994545959342402\n",
            "Iteration 68, Loss: 0.01143317313684464\n",
            "Iteration 69, Loss: 0.010059408822634995\n",
            "Iteration 70, Loss: 0.008850711815103213\n",
            "Iteration 71, Loss: 0.0077872455691811795\n",
            "Iteration 72, Loss: 0.006851560791217683\n",
            "Iteration 73, Loss: 0.006028304125170024\n",
            "Iteration 74, Loss: 0.005303968530948593\n",
            "Iteration 75, Loss: 0.004666664214721105\n",
            "Iteration 76, Loss: 0.004105937307356111\n",
            "Iteration 77, Loss: 0.003612583317356725\n",
            "Iteration 78, Loss: 0.003178510206825254\n",
            "Iteration 79, Loss: 0.0027965930250611172\n",
            "Iteration 80, Loss: 0.002460565405431671\n",
            "Iteration 81, Loss: 0.0021649136170684936\n",
            "Iteration 82, Loss: 0.0019047862813915623\n",
            "Iteration 83, Loss: 0.001675914635903706\n",
            "Iteration 84, Loss: 0.001474543899639681\n",
            "Iteration 85, Loss: 0.0012973685797888121\n",
            "Iteration 86, Loss: 0.001141481620962891\n",
            "Iteration 87, Loss: 0.0010043259246094074\n",
            "Iteration 88, Loss: 0.0008836498995188997\n",
            "Iteration 89, Loss: 0.0007774742196062873\n",
            "Iteration 90, Loss: 0.0006840561769726868\n",
            "Iteration 91, Loss: 0.0006018625743085531\n",
            "Iteration 92, Loss: 0.000529545142648008\n",
            "Iteration 93, Loss: 0.0004659170767124235\n",
            "Iteration 94, Loss: 0.00040993443030225914\n",
            "Iteration 95, Loss: 0.00036067836927467927\n",
            "Iteration 96, Loss: 0.0003173407081270421\n",
            "Iteration 97, Loss: 0.0002792102567001265\n",
            "Iteration 98, Loss: 0.0002456615527775961\n",
            "Iteration 99, Loss: 0.00021614372309699476\n",
            "Iteration 100, Loss: 0.0001901729121621426\n",
            "Iteration 101, Loss: 0.00016732236488384227\n",
            "Iteration 102, Loss: 0.00014721756299453762\n",
            "Iteration 103, Loss: 0.00012952856321770534\n",
            "Iteration 104, Loss: 0.00011396499500746135\n",
            "Iteration 105, Loss: 0.00010027141542742063\n",
            "Iteration 106, Loss: 8.822318210541001e-05\n",
            "Iteration 107, Loss: 7.762252838253687e-05\n",
            "Iteration 108, Loss: 6.82957686907884e-05\n",
            "Iteration 109, Loss: 6.008968331311521e-05\n",
            "Iteration 110, Loss: 5.286947853239571e-05\n",
            "Iteration 111, Loss: 4.65168888304493e-05\n",
            "Iteration 112, Loss: 4.092765743762347e-05\n",
            "Iteration 113, Loss: 3.6009887244728215e-05\n",
            "Iteration 114, Loss: 3.168308464558574e-05\n",
            "Iteration 115, Loss: 2.7876240123093025e-05\n",
            "Iteration 116, Loss: 2.4526707861900252e-05\n",
            "Iteration 117, Loss: 2.1579701119261262e-05\n",
            "Iteration 118, Loss: 1.8986748950176478e-05\n",
            "Iteration 119, Loss: 1.6705392246258687e-05\n",
            "Iteration 120, Loss: 1.4698120139337557e-05\n",
            "Iteration 121, Loss: 1.2932049501405156e-05\n",
            "Iteration 122, Loss: 1.1378173609139993e-05\n",
            "Iteration 123, Loss: 1.0011028661216681e-05\n",
            "Iteration 124, Loss: 8.808146256966508e-06\n",
            "Iteration 125, Loss: 7.749807170678683e-06\n",
            "Iteration 126, Loss: 6.818601946438655e-06\n",
            "Iteration 127, Loss: 5.999325157040919e-06\n",
            "Iteration 128, Loss: 5.278462969410343e-06\n",
            "Iteration 129, Loss: 4.644218769072953e-06\n",
            "Iteration 130, Loss: 4.086179412185025e-06\n",
            "Iteration 131, Loss: 3.595220353613299e-06\n",
            "Iteration 132, Loss: 3.1632189426376917e-06\n",
            "Iteration 133, Loss: 2.7831573174229476e-06\n",
            "Iteration 134, Loss: 2.4487387415007246e-06\n",
            "Iteration 135, Loss: 2.1545035730943108e-06\n",
            "Iteration 136, Loss: 1.8956161668331733e-06\n",
            "Iteration 137, Loss: 1.66785746048698e-06\n",
            "Iteration 138, Loss: 1.4674575177370337e-06\n",
            "Iteration 139, Loss: 1.2911230845504434e-06\n",
            "Iteration 140, Loss: 1.1359948840900371e-06\n",
            "Iteration 141, Loss: 9.995012884301112e-07\n",
            "Iteration 142, Loss: 8.794059850894978e-07\n",
            "Iteration 143, Loss: 7.73738680207357e-07\n",
            "Iteration 144, Loss: 6.807652713748267e-07\n",
            "Iteration 145, Loss: 5.989725687637021e-07\n",
            "Iteration 146, Loss: 5.269973711052699e-07\n",
            "Iteration 147, Loss: 4.6367636916447547e-07\n",
            "Iteration 148, Loss: 4.079594064391233e-07\n",
            "Iteration 149, Loss: 3.589462485525125e-07\n",
            "Iteration 150, Loss: 3.158147052186083e-07\n",
            "Iteration 151, Loss: 2.7786462500342844e-07\n",
            "Iteration 152, Loss: 2.4447752807800465e-07\n",
            "Iteration 153, Loss: 2.1510768882036626e-07\n",
            "Iteration 154, Loss: 1.892611254126917e-07\n",
            "Iteration 155, Loss: 1.6651672105941354e-07\n",
            "Iteration 156, Loss: 1.4650970892272678e-07\n",
            "Iteration 157, Loss: 1.2890803113279003e-07\n",
            "Iteration 158, Loss: 1.1341571100783774e-07\n",
            "Iteration 159, Loss: 9.978836969812208e-08\n",
            "Iteration 160, Loss: 8.779778427447647e-08\n",
            "Iteration 161, Loss: 7.724909259094786e-08\n",
            "Iteration 162, Loss: 6.796810648041798e-08\n",
            "Iteration 163, Loss: 5.97994706521137e-08\n",
            "Iteration 164, Loss: 5.261519865689338e-08\n",
            "Iteration 165, Loss: 4.6293846536076486e-08\n",
            "Iteration 166, Loss: 4.073045361221745e-08\n",
            "Iteration 167, Loss: 3.5837156592179555e-08\n",
            "Iteration 168, Loss: 3.152978864740323e-08\n",
            "Iteration 169, Loss: 2.7741177895541605e-08\n",
            "Iteration 170, Loss: 2.4408590489804116e-08\n",
            "Iteration 171, Loss: 2.1476079840072075e-08\n",
            "Iteration 172, Loss: 1.889591518770306e-08\n",
            "Iteration 173, Loss: 1.6625445250714398e-08\n",
            "Iteration 174, Loss: 1.4626644099036406e-08\n",
            "Iteration 175, Loss: 1.2869068468625077e-08\n",
            "Iteration 176, Loss: 1.132365267581705e-08\n",
            "Iteration 177, Loss: 9.962970327988955e-09\n",
            "Iteration 178, Loss: 8.76560013457745e-09\n",
            "Iteration 179, Loss: 7.712316644113514e-09\n",
            "Iteration 180, Loss: 6.785903626216421e-09\n",
            "Iteration 181, Loss: 5.970978709006616e-09\n",
            "Iteration 182, Loss: 5.252749165633319e-09\n",
            "Iteration 183, Loss: 4.622261009220393e-09\n",
            "Iteration 184, Loss: 4.0663985730454466e-09\n",
            "Iteration 185, Loss: 3.5782690961471883e-09\n",
            "Iteration 186, Loss: 3.1480872302109587e-09\n",
            "Iteration 187, Loss: 2.769700051878304e-09\n",
            "Iteration 188, Loss: 2.4367687356593497e-09\n",
            "Iteration 189, Loss: 2.144341028846081e-09\n",
            "Iteration 190, Loss: 1.8866295768662423e-09\n",
            "Iteration 191, Loss: 1.6598107871186155e-09\n",
            "Iteration 192, Loss: 1.4604247076237892e-09\n",
            "Iteration 193, Loss: 1.2848133885140354e-09\n",
            "Iteration 194, Loss: 1.130764225010497e-09\n",
            "Iteration 195, Loss: 9.948016799656868e-10\n",
            "Iteration 196, Loss: 8.752691733290546e-10\n",
            "Iteration 197, Loss: 7.69790006112388e-10\n",
            "Iteration 198, Loss: 6.774630366247155e-10\n",
            "Iteration 199, Loss: 5.959184979599569e-10\n",
            "Final weights:\n",
            " [[-0.00698895 -0.0139779  -0.02096685 -0.0279558 ]\n",
            " [ 0.25975286  0.11950571 -0.02074143 -0.16098857]\n",
            " [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\n",
            "Final biases:\n",
            " [-0.00698895 -0.04024714 -0.06451539]\n"
          ]
        }
      ],
      "source": [
        "inputs = np.array([1, 2, 3, 4])\n",
        "\n",
        "weights = np.array([\n",
        "    [0.1, 0.2, 0.3, 0.4],\n",
        "    [0.5, 0.6, 0.7, 0.8],\n",
        "    [0.9, 1.0, 1.1, 1.2]\n",
        "])\n",
        "\n",
        "biases = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return np.where(x > 0, 1, 0)\n",
        "\n",
        "for iteration in range(200):\n",
        "\n",
        "  z = np.dot(weights, inputs) + biases\n",
        "  a = relu(z)\n",
        "  y = np.sum(a)\n",
        "\n",
        "  loss = y ** 2\n",
        "\n",
        "  dL_dy = 2 * y\n",
        "  dy_da = np.ones_like(a)\n",
        "  \n",
        "  dL_da = dL_dy * dy_da\n",
        "  da_dz = relu_derivative(z)\n",
        "  dL_dz = dL_da * da_dz\n",
        "  dL_dW = np.outer(dL_dz, inputs)\n",
        "  dL_db = dL_dz\n",
        "\n",
        "  weights -= learning_rate * dL_dW\n",
        "  biases -= learning_rate * dL_db\n",
        "\n",
        "  print(f\"Iteration {iteration}, Loss: {loss}\")\n",
        "\n",
        "print(\"Final weights:\\n\", weights)\n",
        "print(\"Final biases:\\n\", biases)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
